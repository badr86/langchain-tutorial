import { ConversationSummaryMemory } from 'langchain/memory';
import { ConversationChain } from 'langchain/chains';
import { ChatOpenAI } from '@langchain/openai';
import dotenv from 'dotenv';

// Load environment variables
dotenv.config();

/**
 * Conversation Summary Memory Demo
 * 
 * This demo shows how to summarize old conversations to maintain key information.
 * Summary memory compresses conversation history while preserving important context.
 */
async function conversationSummaryMemoryDemo() {
    console.log('üöÄ Executing Conversation Summary Memory Demo...');
    console.log('=' .repeat(60));

    console.log('üìù Conversation Summary Memory Overview:');
    console.log('   ‚Ä¢ Summarizes old conversation parts automatically');
    console.log('   ‚Ä¢ Maintains key information while reducing token usage');
    console.log('   ‚Ä¢ Uses LLM to create intelligent summaries');
    console.log('   ‚Ä¢ Ideal for very long conversations');

    // Initialize the model
    const model = new ChatOpenAI({
        apiKey: process.env.OPENAI_API_KEY,
        temperature: 0.7,
        modelName: 'gpt-3.5-turbo'
    });

    console.log('\nü§ñ Model Configuration:');
    console.log('   ‚Ä¢ Model: gpt-3.5-turbo');
    console.log('   ‚Ä¢ Temperature: 0.7');
    console.log('   ‚Ä¢ API Key:', process.env.OPENAI_API_KEY ? '‚úÖ Found' : '‚ùå Missing');

    if (!process.env.OPENAI_API_KEY) {
        console.log('\n‚ùå OpenAI API Key required for Summary Memory');
        console.log('üí° Summary memory needs LLM access to generate summaries');
        console.log('üìù Showing conceptual demonstration instead...');
        
        // Show conceptual demonstration
        console.log('\nüé≠ Conceptual Summary Memory Workflow:');
        console.log('1. üë§ User: "I\'m planning a trip to Japan next month"');
        console.log('2. ü§ñ AI: "That sounds exciting! What cities are you planning to visit?"');
        console.log('3. üë§ User: "Tokyo and Kyoto mainly"');
        console.log('4. ü§ñ AI: "Great choices! Tokyo offers modern city life..."');
        console.log('5. üë§ User: "I love Japanese cuisine, especially sushi"');
        console.log('6. ü§ñ AI: "You\'ll find amazing sushi in both cities..."');
        
        console.log('\nüìù After several more exchanges, summary memory would create:');
        console.log('üí≠ Summary: "User is planning a trip to Japan (Tokyo & Kyoto) next month. Loves Japanese cuisine, especially sushi. Interested in cultural experiences and food recommendations."');
        
        console.log('\n‚úÖ This summary replaces the full conversation history while preserving key context.');
        return;
    }

    // Create conversation summary memory
    const memory = new ConversationSummaryMemory({
        memoryKey: "chat_history",
        llm: model,
        returnMessages: true,
    });

    console.log('\nüíæ Summary Memory Configuration:');
    console.log('   ‚Ä¢ Memory Key: chat_history');
    console.log('   ‚Ä¢ LLM: gpt-3.5-turbo (for summarization)');
    console.log('   ‚Ä¢ Return Messages: true');
    console.log('   ‚Ä¢ Auto-summarization: enabled');

    // Create conversation chain with summary memory
    const conversationChain = new ConversationChain({
        llm: model,
        memory: memory,
        verbose: true,
    });

    console.log('\nüîó Conversation Chain Created with Summary Memory');

    // Simulate a longer conversation that would benefit from summarization
    const longConversation = [
        {
            input: "I'm planning a trip to Japan next month.",
            description: "Initial travel planning"
        },
        {
            input: "I'm particularly interested in visiting Tokyo and Kyoto.",
            description: "Specific destination preferences"
        },
        {
            input: "I love Japanese cuisine, especially sushi and ramen.",
            description: "Food preferences"
        },
        {
            input: "My budget is around $3000 for the entire trip.",
            description: "Budget constraints"
        },
        {
            input: "I'm also interested in traditional temples and gardens.",
            description: "Cultural interests"
        },
        {
            input: "What's the best time to visit for cherry blossoms?",
            description: "Seasonal timing question"
        },
        {
            input: "Should I book accommodations in advance?",
            description: "Practical travel planning"
        },
        {
            input: "Can you give me some travel recommendations based on everything I've told you?",
            description: "Comprehensive request requiring full context"
        }
    ];

    console.log('\nüó£Ô∏è Starting Long Conversation Simulation...');
    console.log('üìè This conversation will demonstrate automatic summarization');
    
    for (let i = 0; i < longConversation.length; i++) {
        const step = longConversation[i];
        
        console.log(`\n${'='.repeat(50)}`);
        console.log(`üí¨ Conversation Step ${i + 1}/${longConversation.length}`);
        console.log(`${'='.repeat(50)}`);
        
        console.log('üë§ Human:', step.input);
        console.log('üìù Context:', step.description);
        
        try {
            console.log('üîÑ Processing with summary memory...');
            
            const response = await conversationChain.call({
                input: step.input
            });
            
            console.log('ü§ñ AI Response:', response.response.substring(0, 200) + '...');
            
            // Show current memory state
            const memoryVariables = await memory.loadMemoryVariables({});
            
            console.log('üß† Memory State:');
            if (memoryVariables.chat_history) {
                console.log(`   Current summary length: ${memoryVariables.chat_history.length} characters`);
                console.log('   Summary preview:', memoryVariables.chat_history.substring(0, 150) + '...');
            }
            
            // Show summarization in action
            if (i >= 3) { // After a few exchanges
                console.log('üìù Summary Memory is actively compressing conversation history');
                console.log('üí° Key information preserved while reducing token usage');
            }
            
        } catch (error) {
            console.log('‚ùå Error in conversation:', error.message);
        }
        
        // Delay for readability
        await new Promise(resolve => setTimeout(resolve, 1500));
    }

    // Demonstrate summary quality
    console.log(`\n${'='.repeat(60)}`);
    console.log('üìä Summary Quality Analysis');
    console.log(`${'='.repeat(60)}`);

    try {
        const finalMemory = await memory.loadMemoryVariables({});
        
        console.log('üìù Final Conversation Summary:');
        console.log(finalMemory.chat_history);
        
        console.log('\nüîç Summary Analysis:');
        console.log('   ‚Ä¢ Key information preserved: ‚úÖ');
        console.log('   ‚Ä¢ Context compression achieved: ‚úÖ');
        console.log('   ‚Ä¢ Conversation flow maintained: ‚úÖ');
        console.log('   ‚Ä¢ Token usage optimized: ‚úÖ');
        
    } catch (error) {
        console.log('‚ùå Error analyzing summary:', error.message);
    }

    // Compare memory types
    console.log(`\n${'='.repeat(60)}`);
    console.log('‚öñÔ∏è Memory Type Comparison');
    console.log(`${'='.repeat(60)}`);

    console.log('üìä Summary Memory vs Other Types:');
    console.log('\nüß† Buffer Memory:');
    console.log('   ‚Ä¢ Stores: Complete conversation history');
    console.log('   ‚Ä¢ Memory usage: Grows linearly');
    console.log('   ‚Ä¢ Context: Full, no loss');
    console.log('   ‚Ä¢ Best for: Short conversations');

    console.log('\nü™ü Window Memory:');
    console.log('   ‚Ä¢ Stores: Last K message pairs');
    console.log('   ‚Ä¢ Memory usage: Fixed size');
    console.log('   ‚Ä¢ Context: Recent only');
    console.log('   ‚Ä¢ Best for: Long chats, recent focus');

    console.log('\nüìù Summary Memory:');
    console.log('   ‚Ä¢ Stores: Intelligent summary');
    console.log('   ‚Ä¢ Memory usage: Compressed');
    console.log('   ‚Ä¢ Context: Key information preserved');
    console.log('   ‚Ä¢ Best for: Very long conversations');

    // Summary Memory characteristics
    console.log(`\n${'='.repeat(60)}`);
    console.log('üìä Summary Memory Characteristics');
    console.log(`${'='.repeat(60)}`);

    console.log('‚úÖ Advantages:');
    console.log('   ‚Ä¢ Handles very long conversations efficiently');
    console.log('   ‚Ä¢ Preserves key information through summarization');
    console.log('   ‚Ä¢ Reduces token costs for long interactions');
    console.log('   ‚Ä¢ Intelligent compression maintains context');
    console.log('   ‚Ä¢ Scales well with conversation length');

    console.log('\n‚ö†Ô∏è Considerations:');
    console.log('   ‚Ä¢ Requires additional LLM calls for summarization');
    console.log('   ‚Ä¢ May lose some conversational nuances');
    console.log('   ‚Ä¢ Summary quality depends on LLM capabilities');
    console.log('   ‚Ä¢ Slight delay for summarization processing');
    console.log('   ‚Ä¢ More complex than simple buffer approaches');

    console.log('\nüéØ Best Use Cases:');
    console.log('   ‚Ä¢ Extended customer support sessions');
    console.log('   ‚Ä¢ Long-form educational conversations');
    console.log('   ‚Ä¢ Multi-session conversations');
    console.log('   ‚Ä¢ Applications with strict token budgets');
    console.log('   ‚Ä¢ Complex problem-solving discussions');

    console.log('\nüîß Configuration Tips:');
    console.log('   ‚Ä¢ Use same model for memory and conversation');
    console.log('   ‚Ä¢ Monitor summary quality regularly');
    console.log('   ‚Ä¢ Consider hybrid approaches for critical applications');
    console.log('   ‚Ä¢ Test summarization with domain-specific content');

    console.log('\n‚úÖ Conversation Summary Memory Demo completed!');
    console.log('üí° Key takeaways:');
    console.log('   ‚Ä¢ Summary memory compresses conversation history intelligently');
    console.log('   ‚Ä¢ Preserves key information while reducing token usage');
    console.log('   ‚Ä¢ Ideal for very long conversations');
    console.log('   ‚Ä¢ Requires LLM access for summarization');
    console.log('   ‚Ä¢ Balances context preservation with efficiency');
}

// Execute the demo
conversationSummaryMemoryDemo().catch(console.error);
