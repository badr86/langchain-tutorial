{
  "prompts": [
    {
      "id": "basic-prompt",
      "title": "Basic Prompt Template",
      "description": "Simple variable substitution in prompt templates",
      "category": "prompts",
      "codeSnippet": "import { PromptTemplate } from '@langchain/core/prompts';\n\n// Step 1: Create a prompt template with variables\nconst template = \"Tell me a {adjective} joke about {topic}.\";\nconst prompt = new PromptTemplate({\n    template: template,\n    inputVariables: [\"adjective\", \"topic\"],\n});\n\nconsole.log('ðŸ“ Template created:', template);\nconsole.log('ðŸ”§ Input variables:', prompt.inputVariables);\n\n// Step 2: Format the prompt with actual values\nconst formattedPrompt = await prompt.format({\n    adjective: \"funny\",\n    topic: \"programming\"\n});\n\nconsole.log('âœ¨ Formatted prompt:', formattedPrompt);",
      "demoFunction": "basicPromptTemplateDemo",
      "demoFile": "src/demos/prompts/basic-prompt-template.js",
      "capabilities": {
        "whatItCanDo": [
          "Replace variables in text templates with actual values",
          "Create dynamic prompts from static templates",
          "Validate input variables before formatting",
          "Generate consistent prompt structures",
          "Support multiple variable substitutions"
        ],
        "bestUseCases": [
          "Creating personalized messages or content",
          "Building dynamic query generators",
          "Standardizing prompt formats across applications",
          "Rapid prototyping of LLM interactions",
          "Educational content generation"
        ],
        "limitations": [
          "No complex logic or conditional formatting",
          "Limited to simple string substitution",
          "No built-in validation of variable content",
          "Cannot handle nested or complex data structures"
        ],
        "keyTakeaways": [
          "Foundation of all LangChain prompt engineering",
          "Simple but powerful for basic use cases",
          "Essential building block for more complex prompts",
          "Easy to understand and implement",
          "Great starting point for learning LangChain"
        ]
      }
    },
    {
      "id": "chat-prompt",
      "title": "Chat Prompt Template",
      "description": "Structured conversation prompts with system and human messages",
      "category": "prompts",
      "codeSnippet": "import { ChatPromptTemplate } from '@langchain/core/prompts';\n\n// Step 1: Create a chat prompt template with system and human messages\nconst chatPrompt = ChatPromptTemplate.fromMessages([\n    [\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}. Provide accurate translations and explain any cultural context when relevant.\"],\n    [\"human\", \"{text}\"]\n]);\n\nconsole.log('ðŸ“ Chat prompt template created with system and human messages');\nconsole.log('ðŸ”§ Input variables:', chatPrompt.inputVariables);\n\n// Step 2: Format the chat prompt with actual values\nconst formattedMessages = await chatPrompt.formatMessages({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming and creating innovative solutions.\"\n});\n\nconsole.log('âœ¨ Formatted chat messages:');\nformattedMessages.forEach((message, index) => {\n    console.log(`   ${index + 1}. ${message._getType()}: ${message.content}`);\n});",
      "demoFunction": "chatPromptTemplateDemo",
      "demoFile": "src/demos/prompts/chat-prompt-template.js",
      "capabilities": {
        "whatItCanDo": [
          "Create structured conversations with system and user messages",
          "Define different roles for each message (system, human, AI)",
          "Format multi-turn conversations with context",
          "Support variable substitution in conversation templates",
          "Maintain conversation structure and flow"
        ],
        "bestUseCases": [
          "Building chatbots and conversational AI",
          "Creating role-based interactions",
          "Multi-turn conversation systems",
          "Customer support automation",
          "Educational tutoring systems"
        ],
        "limitations": [
          "Limited to predefined message roles",
          "No automatic conversation state management",
          "Cannot handle complex conversation branching",
          "Requires manual conversation flow control"
        ],
        "keyTakeaways": [
          "Essential for building conversational AI applications",
          "Provides structure to chat-based interactions",
          "Foundation for more complex conversation systems",
          "Better than basic prompts for chat scenarios",
          "Supports modern LLM conversation patterns"
        ]
      }
    },
    {
      "id": "complex-prompt",
      "title": "Complex Prompt Template",
      "description": "Multi-step prompts with detailed instructions and examples",
      "category": "prompts",
      "codeSnippet": "import { PromptTemplate } from '@langchain/core/prompts';\n\n// Step 1: Create a complex multi-variable prompt template\nconst complexTemplate = `You are an expert {role} with {experience} years of experience.\n\nTask: {task}\n\nContext:\n{context}\n\nInstructions:\n1. Analyze the given information thoroughly\n2. Provide a detailed response with specific examples\n3. Include both advantages and potential challenges\n4. Conclude with actionable recommendations\n5. Format your response professionally\n\nAdditional Requirements:\n- Target audience: {audience}\n- Response length: {length}\n- Include relevant {domain} terminology\n\nResponse:`;\n\nconst prompt = new PromptTemplate({\n    template: complexTemplate,\n    inputVariables: [\"role\", \"experience\", \"task\", \"context\", \"audience\", \"length\", \"domain\"],\n});\n\nconsole.log('ðŸ“ Complex prompt template created with 7 variables');\nconsole.log('ðŸ”§ Input variables:', prompt.inputVariables);",
      "demoFunction": "complexPromptTemplateDemo",
      "demoFile": "src/demos/prompts/complex-prompt-template.js",
      "capabilities": {
        "whatItCanDo": [
          "Create sophisticated multi-section prompts with clear structure",
          "Define expert personas with specific experience levels",
          "Provide step-by-step instructions for complex tasks",
          "Include context and background information",
          "Generate detailed, structured responses"
        ],
        "bestUseCases": [
          "Professional consulting and advisory systems",
          "Complex analysis and reporting tasks",
          "Educational content creation",
          "Technical documentation generation",
          "Strategic planning and decision support"
        ],
        "limitations": [
          "Can become verbose and overwhelming",
          "May produce overly detailed responses",
          "Requires careful prompt engineering",
          "Higher token usage due to complexity"
        ],
        "keyTakeaways": [
          "Structure is key for complex prompts",
          "Clear instructions improve output quality",
          "Expert personas enhance response authority",
          "Balance detail with clarity",
          "Useful for professional applications"
        ]
      }
    },
    {
      "id": "conditional-prompt",
      "title": "Conditional Prompt Template",
      "description": "Dynamic prompts that change based on input conditions",
      "category": "prompts",
      "codeSnippet": "import { PromptTemplate } from '@langchain/core/prompts';\n\nconsole.log('ðŸŽ¯ Advanced Conditional Prompt Template Demo');\nconsole.log('ðŸ”§ Building dynamic prompts based on multiple conditions...');\n\n// Advanced conditional prompt with multiple parameters\nconst createAdvancedConditionalPrompt = (context) => {\n    const { audience, domain, complexity, includeExamples } = context;\n    \n    let template = `You are explaining {topic} to a ${audience} audience`;\n    \n    if (domain) {\n        template += ` in the context of ${domain}`;\n    }\n    \n    if (complexity === 'high') {\n        template += '. Provide detailed technical analysis';\n    } else if (complexity === 'low') {\n        template += '. Keep explanations simple and accessible';\n    }\n    \n    if (includeExamples) {\n        template += '. Include practical examples and use cases';\n    }\n    \n    template += '. Your explanation:';\n    \n    return PromptTemplate.fromTemplate(template);\n};\n\n// Test advanced conditional prompt\nconst advancedContext = {\n    audience: 'software developer',\n    domain: 'web development',\n    complexity: 'high',\n    includeExamples: true\n};\n\nconsole.log('ðŸ›ï¸ Advanced context:', JSON.stringify(advancedContext, null, 2));\n\nconst advancedPrompt = createAdvancedConditionalPrompt(advancedContext);\nconst advancedFormatted = await advancedPrompt.format({ topic: 'React hooks' });\n\nconsole.log('ðŸ“‹ Advanced Formatted Prompt:');\nconsole.log(`   ${advancedFormatted}`);",
      "demoFunction": "conditionalPromptTemplateDemo",
      "demoFile": "src/demos/prompts/conditional-prompt-template.js",
      "capabilities": {
        "whatItCanDo": [
          "Dynamically select prompt templates based on conditions",
          "Adapt communication style to different audiences",
          "Create branching logic for prompt selection",
          "Provide fallback templates for unknown conditions",
          "Customize responses based on user characteristics"
        ],
        "bestUseCases": [
          "Educational platforms with different skill levels",
          "Adaptive user interfaces",
          "Personalized content generation",
          "Multi-audience documentation systems",
          "Context-aware chatbots"
        ],
        "limitations": [
          "Requires predefined conditions and templates",
          "Limited to simple conditional logic",
          "No machine learning-based adaptation",
          "Manual template management needed"
        ],
        "keyTakeaways": [
          "Enables personalized AI interactions",
          "Simple but effective for audience targeting",
          "Foundation for adaptive systems",
          "Improves user experience through customization",
          "Easy to implement and maintain"
        ]
      }
    },
    {
      "id": "few-shot-prompt",
      "title": "Few-Shot Prompt Template",
      "description": "Learning from examples with few-shot prompting techniques",
      "category": "prompts",
      "codeSnippet": "import { PromptTemplate, FewShotPromptTemplate } from '@langchain/core/prompts';\n\n// Step 1: Define examples for few-shot learning\nconst examples = [\n    { input: \"happy\", output: \"sad\" },\n    { input: \"tall\", output: \"short\" },\n    { input: \"fast\", output: \"slow\" },\n    { input: \"bright\", output: \"dark\" },\n    { input: \"hot\", output: \"cold\" }\n];\n\nconsole.log('ðŸ“š Created examples for antonym generation:');\nexamples.forEach((example, index) => {\n    console.log(`   ${index + 1}. ${example.input} â†’ ${example.output}`);\n});\n\n// Step 2: Create the example prompt template\nconst examplePrompt = PromptTemplate.fromTemplate(\n    \"Input: {input}\\nOutput: {output}\"\n);\n\n// Step 3: Create the few-shot prompt template\nconst fewShotPrompt = new FewShotPromptTemplate({\n    examples: examples,\n    examplePrompt: examplePrompt,\n    prefix: \"Give the antonym (opposite) of the following words. Follow the pattern shown in the examples:\",\n    suffix: \"Input: {adjective}\\nOutput:\",\n    inputVariables: [\"adjective\"],\n});\n\nconsole.log('ðŸŽ¯ Few-shot prompt template created with prefix, examples, and suffix');",
      "demoFunction": "fewShotPromptTemplateDemo",
      "demoFile": "src/demos/prompts/few-shot-prompt-template.js",
      "capabilities": {
        "whatItCanDo": [
          "Teach AI models through example-based learning",
          "Provide consistent formatting for input-output pairs",
          "Demonstrate patterns through multiple examples",
          "Improve model performance on specific tasks",
          "Create structured learning templates"
        ],
        "bestUseCases": [
          "Classification and categorization tasks",
          "Pattern recognition and completion",
          "Format standardization",
          "Teaching specific response styles",
          "Domain-specific task training"
        ],
        "limitations": [
          "Requires high-quality examples",
          "Limited by the number and diversity of examples",
          "May not generalize well beyond examples",
          "Increases prompt length and token usage"
        ],
        "keyTakeaways": [
          "Examples are powerful teachers for AI models",
          "Quality over quantity in example selection",
          "Effective for specific, well-defined tasks",
          "Foundation of in-context learning",
          "Essential technique for prompt engineering"
        ]
      }
    },
    {
      "id": "openai-prompt",
      "title": "OpenAI Integration",
      "description": "Using prompt templates with OpenAI models for real responses",
      "category": "prompts",
      "codeSnippet": "const model = new OpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n    temperature: 0.7,\n});\n\nconst prompt = PromptTemplate.fromTemplate(\n    \"Write a {length} {genre} story about {character}.\"\n);\n\nconst chain = prompt.pipe(model);\n\nconst result = await chain.invoke({\n    length: \"short\",\n    genre: \"mystery\",\n    character: \"a detective cat\"\n});",
      "demoFunction": "promptTemplateWithOpenAIDemo",
      "demoFile": "src/demos/prompts/openai-prompt-template.js",
      "capabilities": {
        "whatItCanDo": [
          "Connect prompt templates directly to OpenAI models",
          "Generate real AI responses using templates",
          "Control model parameters like temperature",
          "Create end-to-end prompt-to-response pipelines",
          "Handle API authentication and configuration"
        ],
        "bestUseCases": [
          "Production AI applications",
          "Content generation systems",
          "Interactive AI demos",
          "Automated writing assistants",
          "Creative AI applications"
        ],
        "limitations": [
          "Requires valid OpenAI API key",
          "Subject to API rate limits and costs",
          "Dependent on OpenAI service availability",
          "Response quality varies with prompt design"
        ],
        "keyTakeaways": [
          "Bridge between templates and real AI responses",
          "Essential for production LangChain applications",
          "Demonstrates complete prompt-to-response workflow",
          "Foundation for building AI-powered features",
          "Shows practical LangChain implementation"
        ]
      }
    }
  ],
  "chains": [
    {
      "id": "basic-chain",
      "title": "Basic LLMChain",
      "description": "Traditional LangChain LLMChain usage with OpenAI",
      "category": "chains",
      "codeSnippet": "import { PromptTemplate } from '@langchain/core/prompts';\nimport { ChatOpenAI } from '@langchain/openai';\nimport { LLMChain } from 'langchain/chains';\n\n// Step 1: Create a prompt template\nconst prompt = new PromptTemplate({\n    template: \"Write a {length} {style} story about {topic}.\",\n    inputVariables: [\"length\", \"style\", \"topic\"],\n});\n\nconsole.log('ðŸ“ Prompt template created');\nconsole.log('ðŸ”§ Template:', prompt.template);\n\n// Step 2: Initialize the LLM\nconst model = new ChatOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n    temperature: 0.7,\n    modelName: 'gpt-3.5-turbo',\n});\n\nconsole.log('ðŸ¤– ChatOpenAI model initialized');\n\n// Step 3: Create the LLM Chain\nconst chain = new LLMChain({\n    llm: model,\n    prompt: prompt,\n});\n\nconsole.log('â›“ï¸ LLM Chain created');\n\n// Step 4: Execute the chain\nconst result = await chain.call({\n    topic: \"a time-traveling cat\",\n    style: \"humorous\",\n    length: \"short\"\n});\n\nconsole.log('âœ¨ Generated Story:', result.text);",
      "demoFunction": "basicLLMChainDemo",
      "demoFile": "src/demos/chains/basic-llm-chain.js",
      "capabilities": {
        "whatItCanDo": [
          "Combine prompts and language models into reusable chains",
          "Execute prompt-model workflows with single method calls",
          "Handle input validation and output processing",
          "Provide consistent interface for LLM interactions",
          "Support various LLM providers through unified API"
        ],
        "bestUseCases": [
          "Simple AI text generation tasks",
          "Standardized LLM workflows",
          "Rapid prototyping of AI features",
          "Educational LangChain demonstrations",
          "Building blocks for complex applications"
        ],
        "limitations": [
          "Limited to single-step operations",
          "No built-in error handling or retries",
          "Cannot handle complex multi-step workflows",
          "Legacy approach compared to LCEL"
        ],
        "keyTakeaways": [
          "Foundation of traditional LangChain architecture",
          "Simple but effective for basic use cases",
          "Good starting point for learning chains",
          "Being superseded by LCEL for new projects",
          "Still useful for simple applications"
        ]
      }
    },
    {
      "id": "simple-lcel",
      "title": "Simple LCEL Chain",
      "description": "Modern LangChain Expression Language with pipe operators",
      "category": "chains",
      "codeSnippet": "import { ChatPromptTemplate } from '@langchain/core/prompts';\nimport { ChatOpenAI } from '@langchain/openai';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\n\n// Initialize the model\nconst model = new ChatOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n    temperature: 0.7,\n    modelName: 'gpt-3.5-turbo'\n});\n\nconsole.log('ðŸ¤– Model Configuration:');\nconsole.log('   â€¢ Model: gpt-3.5-turbo');\nconsole.log('   â€¢ Temperature: 0.7');\n\n// Create a simple translation prompt\nconst translationPrompt = ChatPromptTemplate.fromTemplate(\n    \"Translate the following text to {target_language}: {text}\"\n);\n\nconsole.log('ðŸ“ Translation Prompt Template:');\nconsole.log('   \"Translate the following text to {target_language}: {text}\"');\n\n// LCEL Chain using pipe operator - this is the key feature!\nconsole.log('ðŸ”— Building LCEL Chain with pipe operators:');\nconsole.log('   chain = prompt.pipe(model).pipe(outputParser)');\n\nconst translationChain = translationPrompt.pipe(model).pipe(new StringOutputParser());\n\n// Execute the chain\nconst result = await translationChain.invoke({\n    text: \"Hello, how are you today?\",\n    target_language: \"Spanish\"\n});\n\nconsole.log('âœ… Translation result:', result);",
      "demoFunction": "simpleLCELChainDemo",
      "demoFile": "src/demos/chains/simple-lcel-chain.js",
      "capabilities": {
        "whatItCanDo": [
          "Chain multiple components using pipe operators",
          "Create readable and maintainable chain workflows",
          "Automatically handle data flow between components",
          "Support async operations throughout the chain",
          "Enable easy composition of complex workflows"
        ],
        "bestUseCases": [
          "Text processing and transformation pipelines",
          "Translation and language conversion tasks",
          "Content generation with post-processing",
          "Building modular AI applications",
          "Rapid prototyping of LLM workflows"
        ],
        "limitations": [
          "Requires understanding of LCEL syntax",
          "Error handling can be complex in long chains",
          "Debugging multi-step chains can be challenging",
          "Performance overhead with many pipe operations"
        ],
        "keyTakeaways": [
          "LCEL is the modern way to build LangChain applications",
          "Pipe operators make chains readable and intuitive",
          "Great for building production-ready workflows",
          "More flexible than traditional LLMChain approach",
          "Essential skill for modern LangChain development"
        ]
      }
    },
    {
      "id": "complex-lcel",
      "title": "Complex LCEL Chain",
      "description": "Multi-step LCEL chains with RunnableSequence",
      "category": "chains",
      "codeSnippet": "import { ChatPromptTemplate } from '@langchain/core/prompts';\nimport { ChatOpenAI } from '@langchain/openai';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\nimport { RunnableSequence, RunnableMap } from '@langchain/core/runnables';\n\n// Initialize the model\nconst model = new ChatOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n    temperature: 0.7,\n    modelName: 'gpt-3.5-turbo'\n});\n\nconsole.log('ðŸ—ï¸ Building Complex Multi-Step Chain:');\nconsole.log('   Step 1: Generate poem about topic');\nconsole.log('   Step 2: Translate poem to target language');\nconsole.log('   Step 3: Analyze the translated poem');\n\n// Create a complex multi-step chain\nconst complexChain = RunnableSequence.from([\n    // Step 1: Input processing and poem generation\n    {\n        topic: (input) => input.topic,\n        language: (input) => input.language,\n    },\n    {\n        topic: (input) => input.topic,\n        language: (input) => input.language,\n        poem: RunnableSequence.from([\n            ChatPromptTemplate.fromTemplate(\n                \"Write a short, beautiful poem about {topic}. Make it creative and inspiring.\"\n            ),\n            model,\n            new StringOutputParser(),\n        ]),\n    },\n    // Step 2: Translation\n    {\n        topic: (input) => input.topic,\n        language: (input) => input.language,\n        originalPoem: (input) => input.poem,\n        translatedPoem: RunnableSequence.from([\n            ChatPromptTemplate.fromTemplate(\n                \"Translate this poem to {language}, maintaining its poetic beauty and meaning:\\n\\n{poem}\"\n            ),\n            model,\n            new StringOutputParser(),\n        ]),\n    },\n    // Step 3: Analysis\n    {\n        result: RunnableSequence.from([\n            ChatPromptTemplate.fromTemplate(\n                `Analyze this poetry translation:\n\nOriginal Topic: {topic}\nTarget Language: {language}\nOriginal Poem: {originalPoem}\nTranslated Poem: {translatedPoem}\n\nProvide analysis of:\n1. Translation quality\n2. Poetic elements preserved\n3. Cultural adaptation\n4. Overall effectiveness`\n            ),\n            model,\n            new StringOutputParser(),\n        ]),\n        metadata: (input) => ({\n            topic: input.topic,\n            language: input.language,\n            processingSteps: 3,\n            timestamp: new Date().toISOString()\n        })\n    },\n]);\n\n// Execute the chain\nconst result = await complexChain.invoke({\n    topic: \"artificial intelligence\",\n    language: \"Spanish\"\n});\n\nconsole.log('âœ… Complex chain result:', result.result);\nconsole.log('ðŸ“Š Metadata:', result.metadata);",
      "demoFunction": "complexLCELChainDemo",
      "demoFile": "src/demos/chains/complex-lcel-chain.js",
      "capabilities": {
        "whatItCanDo": [
          "Create sophisticated multi-step workflows with RunnableSequence",
          "Handle complex data transformations between steps",
          "Compose nested chains for advanced processing",
          "Manage intermediate results and data flow",
          "Build production-ready multi-stage pipelines"
        ],
        "bestUseCases": [
          "Content creation and transformation workflows",
          "Multi-language processing pipelines",
          "Complex document processing systems",
          "Advanced AI content generation",
          "Enterprise-level automation workflows"
        ],
        "limitations": [
          "Increased complexity and debugging difficulty",
          "Higher computational overhead",
          "Requires careful error handling at each step",
          "Can be overkill for simple tasks"
        ],
        "keyTakeaways": [
          "RunnableSequence enables powerful workflow composition",
          "Essential for building complex AI applications",
          "Requires good understanding of data flow",
          "Great for production systems with multiple processing steps",
          "Demonstrates advanced LCEL capabilities"
        ]
      }
    },
    {
      "id": "custom-functions",
      "title": "LCEL with Custom Functions",
      "description": "Integrating custom logic with RunnableLambda in LCEL chains",
      "category": "chains",
      "codeSnippet": "import { ChatPromptTemplate } from '@langchain/core/prompts';\nimport { ChatOpenAI } from '@langchain/openai';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\nimport { RunnableSequence, RunnableLambda } from '@langchain/core/runnables';\n\n// Initialize the model\nconst model = new ChatOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n    temperature: 0.7,\n    modelName: 'gpt-3.5-turbo'\n});\n\nconsole.log('ðŸ”§ Custom Functions in LCEL:');\nconsole.log('   â€¢ RunnableLambda for custom logic integration');\nconsole.log('   â€¢ Data preprocessing and postprocessing');\nconsole.log('   â€¢ Custom analysis and transformation');\n\n// Create custom analysis function\nconst textAnalyzer = new RunnableLambda({\n    func: (input) => {\n        console.log('ðŸ” Running custom text analysis...');\n        \n        const text = typeof input === 'string' ? input : input.text || '';\n        const words = text.split(/\\s+/).filter(word => word.length > 0);\n        const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);\n        \n        // Sentiment analysis (simple keyword-based)\n        const positiveWords = ['good', 'great', 'excellent', 'amazing', 'wonderful'];\n        const negativeWords = ['bad', 'terrible', 'awful', 'hate', 'worst'];\n        \n        const positiveCount = positiveWords.filter(word => \n            text.toLowerCase().includes(word)\n        ).length;\n        const negativeCount = negativeWords.filter(word => \n            text.toLowerCase().includes(word)\n        ).length;\n        \n        let sentiment = 'neutral';\n        if (positiveCount > negativeCount) sentiment = 'positive';\n        else if (negativeCount > positiveCount) sentiment = 'negative';\n        \n        const analysis = {\n            original: text,\n            wordCount: words.length,\n            sentenceCount: sentences.length,\n            sentiment: sentiment,\n            positiveWords: positiveCount,\n            negativeWords: negativeCount,\n            timestamp: new Date().toISOString()\n        };\n        \n        console.log('ðŸ“Š Analysis completed:', {\n            words: analysis.wordCount,\n            sentences: analysis.sentenceCount,\n            sentiment: analysis.sentiment\n        });\n        \n        return analysis;\n    },\n});\n\n// Create chain with custom analysis\nconst analysisChain = RunnableSequence.from([\n    textAnalyzer,\n    ChatPromptTemplate.fromTemplate(\n        `Text Analysis Results:\nOriginal: {original}\nWord Count: {wordCount}\nSentences: {sentenceCount}\nSentiment: {sentiment}\nPositive Words: {positiveWords}\nNegative Words: {negativeWords}\n\nProvide detailed insights based on this analysis.`\n    ),\n    model,\n    new StringOutputParser(),\n]);\n\n// Execute the chain\nconst result = await analysisChain.invoke(\"This is a great example of wonderful technology!\");\nconsole.log('âœ… Analysis result:', result);",
      "demoFunction": "lcelWithCustomFunctionsDemo",
      "demoFile": "src/demos/chains/lcel-with-custom-functions.js",
      "capabilities": {
        "whatItCanDo": [
          "Integrate custom JavaScript functions into LCEL chains",
          "Perform data preprocessing and analysis",
          "Add business logic between chain steps",
          "Transform data formats and structures",
          "Implement custom validation and filtering"
        ],
        "bestUseCases": [
          "Data preprocessing and cleaning",
          "Custom business logic integration",
          "Text analysis and sentiment detection",
          "Format conversions and transformations",
          "Adding timestamps and metadata"
        ],
        "limitations": [
          "Functions must be synchronous or properly handled",
          "Error handling needs to be implemented manually",
          "Performance depends on custom function efficiency",
          "Debugging can be more complex"
        ],
        "keyTakeaways": [
          "RunnableLambda bridges custom code with LCEL",
          "Essential for adding business logic to chains",
          "Enables powerful data preprocessing capabilities",
          "Great for integrating existing functions",
          "Key to building production-ready applications"
        ]
      }
    },
    {
      "id": "parallel-chains",
      "title": "Parallel LCEL Chains",
      "description": "Running multiple chains in parallel for comprehensive analysis",
      "category": "chains",
      "codeSnippet": "const { PromptTemplate } = require('@langchain/core/prompts');\nconst { ChatOpenAI } = require('@langchain/openai');\nconst { StringOutputParser } = require('@langchain/core/output_parsers');\nconst { RunnableMap } = require('@langchain/core/runnables');\n\n// Step 1: Create multiple prompt templates for different types of content\nconst jokePrompt = PromptTemplate.fromTemplate(\n    \"Tell me a funny, clean joke about {topic}. Make it clever and programming-related.\"\n);\n\nconst factPrompt = PromptTemplate.fromTemplate(\n    \"Give me an interesting and lesser-known fact about {topic}. Include some technical details.\"\n);\n\nconst quotePrompt = PromptTemplate.fromTemplate(\n    \"Provide an inspirational quote related to {topic}. Make it motivational for developers.\"\n);\n\nconst tipPrompt = PromptTemplate.fromTemplate(\n    \"Give me a practical tip or best practice related to {topic} that developers should know.\"\n);\n\nconst topic = \"programming\";\nconsole.log(`ðŸŽ¯ Topic: ${topic}`);\nconsole.log('ðŸ“ Created 4 different prompt templates for parallel execution');\n\n// Step 2: Set up parallel execution\nconst llm = new ChatOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n    temperature: 0.7,\n    modelName: 'gpt-3.5-turbo',\n});\n\n// Create parallel chains using RunnableMap\nconst parallelChain = RunnableMap.from({\n    joke: jokePrompt.pipe(llm).pipe(new StringOutputParser()),\n    fact: factPrompt.pipe(llm).pipe(new StringOutputParser()),\n    quote: quotePrompt.pipe(llm).pipe(new StringOutputParser()),\n    tip: tipPrompt.pipe(llm).pipe(new StringOutputParser())\n});\n\nconsole.log('ðŸ”„ Running 4 chains in parallel...');\nconsole.log('â±ï¸ This should be faster than running them sequentially!');\n\nconst startTime = Date.now();\nconst results = await parallelChain.invoke({ topic });\nconst endTime = Date.now();\nconst duration = endTime - startTime;\n\nconsole.log(`âš¡ Parallel execution completed in ${duration}ms`);\nconsole.log('ðŸ˜‚ JOKE:', results.joke);\nconsole.log('ðŸ’¡ FACT:', results.fact);\nconsole.log('âœ¨ QUOTE:', results.quote);\nconsole.log('ðŸŽ¯ TIP:', results.tip);",
      "demoFunction": "parallelLCELChainsDemo",
      "demoFile": "src/demos/chains/parallel-lcel-chains.js",
      "capabilities": {
        "whatItCanDo": [
          "Execute multiple chains simultaneously for faster processing",
          "Perform comprehensive multi-perspective analysis",
          "Combine results from different processing approaches",
          "Optimize performance through parallel execution",
          "Generate structured outputs with multiple data points"
        ],
        "bestUseCases": [
          "Comprehensive content analysis",
          "Multi-angle research and reporting",
          "Performance optimization for independent tasks",
          "Comparative analysis workflows",
          "Rich data generation for decision making"
        ],
        "limitations": [
          "Higher resource consumption during parallel execution",
          "Complexity in error handling across multiple chains",
          "Potential for inconsistent results between parallel branches",
          "Requires careful coordination of results"
        ],
        "keyTakeaways": [
          "Parallel execution significantly improves performance",
          "Great for comprehensive analysis workflows",
          "Essential for building efficient AI applications",
          "Enables rich, multi-dimensional outputs",
          "Key technique for production systems"
        ]
      }
    },
    {
      "id": "conditional-chain",
      "title": "Conditional LCEL Chain",
      "description": "Dynamic chain selection based on input",
      "category": "chains",
      "codeSnippet": "require('dotenv').config();\nconst { PromptTemplate } = require('@langchain/core/prompts');\nconst { ChatOpenAI } = require('@langchain/openai');\nconst { StringOutputParser } = require('@langchain/core/output_parsers');\nconst { RunnableSequence } = require('@langchain/core/runnables');\n\nconsole.log('ðŸš€ Executing Conditional LCEL Chain Demo...');\nconsole.log('ðŸ”§ Creating conditional chains for different audiences...');\n\n// Step 1: Define different templates for different audiences\nconst createConditionalChain = (audience) => {\n    const templates = {\n        beginner: \"Explain {topic} in simple terms with basic examples that a beginner can understand.\",\n        intermediate: \"Provide a detailed explanation of {topic} with practical applications and some technical details.\",\n        expert: \"Give an advanced analysis of {topic} including edge cases, optimizations, and implementation details.\"\n    };\n    \n    const selectedTemplate = templates[audience] || templates.intermediate;\n    console.log(`ðŸ“ Creating ${audience} chain with template:`, selectedTemplate);\n    \n    const prompt = PromptTemplate.fromTemplate(selectedTemplate);\n    const llm = new ChatOpenAI({\n        apiKey: process.env.OPENAI_API_KEY,\n        temperature: 0.7,\n        modelName: 'gpt-3.5-turbo',\n    });\n    \n    // Create a chain: prompt -> llm -> output parser\n    return RunnableSequence.from([\n        prompt,\n        llm,\n        new StringOutputParser()\n    ]);\n};\n\n// Step 2: Create and execute chains\nconst beginnerChain = createConditionalChain('beginner');\nconst expertChain = createConditionalChain('expert');\n\nconst topic = \"React hooks\";\nconst beginnerResult = await beginnerChain.invoke({ topic });\nconst expertResult = await expertChain.invoke({ topic });\n\nconsole.log('ðŸ‘¶ Beginner Response:', beginnerResult);\nconsole.log('ðŸŽ“ Expert Response:', expertResult);",
      "demoFunction": "conditionalLCELChainDemo",
      "demoFile": "src/demos/chains/conditional-lcel-chain.js",
      "capabilities": {
        "whatItCanDo": [
          "Dynamically select different chain configurations based on input",
          "Adapt processing logic to different user types or contexts",
          "Provide fallback chains for unknown conditions",
          "Create audience-specific response strategies",
          "Enable context-aware chain execution"
        ],
        "bestUseCases": [
          "Adaptive educational content delivery",
          "Multi-audience documentation systems",
          "Context-sensitive AI responses",
          "Personalized user experiences",
          "Dynamic workflow routing"
        ],
        "limitations": [
          "Requires predefined conditions and chain mappings",
          "Limited to simple conditional logic",
          "No machine learning-based adaptation",
          "Manual maintenance of condition-chain relationships"
        ],
        "keyTakeaways": [
          "Enables intelligent chain selection based on context",
          "Essential for building adaptive AI systems",
          "Great for personalization and customization",
          "Foundation for smart routing in complex applications",
          "Combines simplicity with powerful conditional logic"
        ]
      }
    },
    {
      "id": "basic-structured-output",
      "title": "Basic Structured Output",
      "description": "Generate validated JSON output using Zod schemas for consistent data structures",
      "category": "chains",
      "codeSnippet": "import { ChatOpenAI } from '@langchain/openai';\nimport { ChatPromptTemplate } from '@langchain/core/prompts';\nimport { JsonOutputParser } from '@langchain/core/output_parsers';\nimport { RunnableSequence } from '@langchain/core/runnables';\nimport { z } from 'zod';\n\n// Define Zod schema for structured output\nconst PersonSchema = z.object({\n  name: z.string().describe(\"Full name of the person\"),\n  age: z.number().min(0).max(150).describe(\"Age in years\"),\n  occupation: z.string().describe(\"Current job or profession\"),\n  location: z.string().describe(\"City and country where they live\"),\n  skills: z.array(z.string()).describe(\"List of 3-5 key professional skills\"),\n  interests: z.array(z.string()).describe(\"List of 3-5 personal interests\"),\n  personality_traits: z.array(z.string()).describe(\"List of 3-5 personality traits\"),\n  email: z.string().email().describe(\"Professional email address\"),\n  phone: z.string().describe(\"Phone number in format +1-XXX-XXX-XXXX\")\n});\n\n// Initialize components\nconst llm = new ChatOpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  modelName: 'gpt-3.5-turbo',\n  temperature: 0.3, // Lower temperature for consistent output\n  maxTokens: 1000,\n});\n\n// Create prompt template with explicit JSON instructions\nconst personPrompt = ChatPromptTemplate.fromTemplate(`\nGenerate a detailed person profile based on the given description.\n\nDescription: {description}\n\nIMPORTANT: You must respond with valid JSON only.\nPlease provide the information in the following JSON format:\n{format_instructions}\n\nMake sure ALL fields are filled with realistic information.\n`);\n\n// Create parser with schema validation\nconst personParser = new JsonOutputParser(PersonSchema);\n\n// Build the structured output chain\nconst personChain = RunnableSequence.from([\n  {\n    description: (input) => input.description,\n    format_instructions: () => personParser.getFormatInstructions(),\n  },\n  personPrompt,\n  llm,\n  personParser,\n]);\n\n// Execute the chain\nconst result = await personChain.invoke({\n  description: \"A 28-year-old software engineer from San Francisco\"\n});\n\nconsole.log('Generated Person Profile:', result);",
      "demoFunction": "basicStructuredOutputDemo",
      "demoFile": "src/demos/chains/basic-structured-output.js",
      "capabilities": {
        "whatItCanDo": [
          "Generate JSON output with predefined Zod schemas",
          "Validate and parse LLM responses automatically",
          "Ensure type-safe, consistent data structures",
          "Handle person profiles and company analysis",
          "Provide runtime validation and error handling"
        ],
        "bestUseCases": [
          "API responses requiring consistent JSON structure",
          "Data extraction from unstructured text",
          "User profile and business data generation",
          "Form generation and validation systems",
          "Structured content creation workflows"
        ],
        "limitations": [
          "Requires careful prompt engineering for consistent output",
          "JSON parsing can fail with complex or ambiguous requests",
          "Schema validation adds computational overhead",
          "Works best with flatter, simpler schema structures"
        ],
        "keyTakeaways": [
          "Essential for building reliable AI applications with predictable output",
          "Zod schemas provide both validation and TypeScript types",
          "Lower temperature settings improve JSON consistency",
          "Design schemas that match LLM natural output patterns",
          "Foundation for building production-ready AI APIs"
        ]
      }
    },
    {
      "id": "multi-step-structured-pipeline",
      "title": "Multi-Step Structured Pipeline",
      "description": "Chain multiple structured outputs where each step builds upon the previous",
      "category": "chains",
      "codeSnippet": "import { ChatOpenAI } from '@langchain/openai';\nimport { ChatPromptTemplate } from '@langchain/core/prompts';\nimport { JsonOutputParser } from '@langchain/core/output_parsers';\nimport { RunnableSequence } from '@langchain/core/runnables';\nimport { z } from 'zod';\n\n// Step 1 Schema: Product Idea Generation\nconst ProductIdeaSchema = z.object({\n  name: z.string().describe(\"Name of the product\"),\n  description: z.string().describe(\"Brief description of the product\"),\n  category: z.string().describe(\"Product category or industry\"),\n  target_audience: z.array(z.string()).describe(\"List of target audience segments\"),\n  key_features: z.array(z.string()).describe(\"List of 3-5 key product features\"),\n  unique_value_proposition: z.string().describe(\"What makes this product unique\"),\n  estimated_price_range: z.string().describe(\"Expected price range\"),\n  monetization_options: z.array(z.string()).describe(\"Ways to generate revenue\")\n});\n\n// Step 2 Schema: Market Analysis\nconst MarketAnalysisSchema = z.object({\n  market_size: z.string().describe(\"Total addressable market size\"),\n  growth_rate: z.string().describe(\"Annual market growth rate\"),\n  competition_level: z.enum(['Low', 'Medium', 'High']).describe(\"Level of competition\"),\n  key_competitors: z.array(z.string()).describe(\"List of main competitors\"),\n  market_trends: z.array(z.string()).describe(\"Current market trends\"),\n  opportunities: z.array(z.string()).describe(\"Market opportunities\"),\n  challenges: z.array(z.string()).describe(\"Potential challenges\"),\n  success_probability: z.enum(['Low', 'Medium', 'High']).describe(\"Probability of success\"),\n  recommended_strategy: z.string().describe(\"Recommended go-to-market strategy\")\n});\n\n// Initialize LLM\nconst llm = new ChatOpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  modelName: 'gpt-3.5-turbo',\n  temperature: 0.4,\n  maxTokens: 1500\n});\n\n// Step 1: Product Idea Chain\nconst productIdeaChain = RunnableSequence.from([\n  {\n    industry: (input) => input.industry,\n    problem: (input) => input.problem,\n    format_instructions: () => new JsonOutputParser(ProductIdeaSchema).getFormatInstructions()\n  },\n  productIdeaPrompt,\n  llm,\n  new JsonOutputParser(ProductIdeaSchema)\n]);\n\n// Step 2: Market Analysis Chain (uses Step 1 output)\nconst marketAnalysisChain = RunnableSequence.from([\n  {\n    name: (input) => input.name,\n    description: (input) => input.description,\n    category: (input) => input.category,\n    target_audience: (input) => Array.isArray(input.target_audience) ? input.target_audience.join(', ') : 'N/A',\n    key_features: (input) => Array.isArray(input.key_features) ? input.key_features.join(', ') : 'N/A',\n    unique_value_proposition: (input) => input.unique_value_proposition,\n    estimated_price_range: (input) => input.estimated_price_range,\n    monetization_options: (input) => Array.isArray(input.monetization_options) ? input.monetization_options.join(', ') : 'N/A',\n    format_instructions: () => new JsonOutputParser(MarketAnalysisSchema).getFormatInstructions()\n  },\n  marketAnalysisPrompt,\n  llm,\n  new JsonOutputParser(MarketAnalysisSchema)\n]);\n\n// Execute pipeline\nconst productIdea = await productIdeaChain.invoke({\n  industry: \"Health & Wellness\",\n  problem: \"People struggle to maintain consistent exercise routines at home\"\n});\n\nconst marketAnalysis = await marketAnalysisChain.invoke(productIdea);\n\nconsole.log('Pipeline Results:', { productIdea, marketAnalysis });",
      "demoFunction": "multiStepStructuredPipelineDemo",
      "demoFile": "src/demos/chains/multi-step-structured-pipeline.js",
      "capabilities": {
        "whatItCanDo": [
          "Chain multiple structured outputs together",
          "Pass data between pipeline steps automatically",
          "Build complex workflows with validated data flow",
          "Handle multi-stage analysis and processing",
          "Create sequential AI processing pipelines"
        ],
        "bestUseCases": [
          "Product development and market analysis workflows",
          "Multi-stage content creation pipelines",
          "Business analysis and planning processes",
          "Research and recommendation systems",
          "Complex decision-making workflows"
        ],
        "limitations": [
          "Each step depends on the success of previous steps",
          "Longer execution time due to multiple AI calls",
          "Error in any step can break the entire pipeline",
          "Requires careful data flow management between steps"
        ],
        "keyTakeaways": [
          "Perfect for complex workflows requiring multiple AI processing stages",
          "Each step should have focused, well-defined responsibilities",
          "Design schemas that work well together across steps",
          "Essential for building sophisticated AI-powered business processes",
          "Mirrors human decision-making with structured intermediate steps"
        ]
      }
    }
  ],
  "memory": [
    {
      "id": "basic-buffer-memory",
      "title": "Basic Buffer Memory",
      "description": "Store complete conversation history for context",
      "category": "memory",
      "codeSnippet": "import { BufferMemory } from 'langchain/memory';\nimport { ConversationChain } from 'langchain/chains';\nimport { ChatOpenAI } from '@langchain/openai';\nimport { ChatPromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts';\n\n// Initialize the model\nconst model = new ChatOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n    temperature: 0.7,\n    modelName: 'gpt-3.5-turbo'\n});\n\nconsole.log('ðŸ¤– Model Configuration:');\nconsole.log('   â€¢ Model: gpt-3.5-turbo');\nconsole.log('   â€¢ Temperature: 0.7');\n\n// Create buffer memory\nconst memory = new BufferMemory({\n    memoryKey: \"chat_history\",\n    returnMessages: true,\n});\n\nconsole.log('ðŸ’¾ Buffer Memory Configuration:');\nconsole.log('   â€¢ Memory Key: chat_history');\nconsole.log('   â€¢ Return Messages: true');\nconsole.log('   â€¢ Storage: In-memory (temporary)');\n\n// Create conversation chain with memory\nconst conversationChain = new ConversationChain({\n    llm: model,\n    memory: memory,\n    verbose: true,\n});\n\nconsole.log('ðŸ”— Conversation Chain Created with Buffer Memory');\n\n// First interaction\nconst response1 = await conversationChain.call({\n    input: \"Hi, my name is Alice and I love programming.\"\n});\n\nconsole.log('ðŸ‘¤ Human: Hi, my name is Alice and I love programming.');\nconsole.log('ðŸ¤– AI Response:', response1.response);\n\n// Second interaction - AI remembers the context\nconst response2 = await conversationChain.call({\n    input: \"What's my name and what do I love?\"\n});\n\nconsole.log('ðŸ‘¤ Human: What\\'s my name and what do I love?');\nconsole.log('ðŸ¤– AI Response:', response2.response);\n\n// Show current memory state\nconst memoryVariables = await memory.loadMemoryVariables({});\nconsole.log('ðŸ§  Memory State:');\nconsole.log(`   Messages in memory: ${memoryVariables.chat_history.length}`);",
      "demoFunction": "basicBufferMemoryDemo",
      "demoFile": "src/demos/memory/basic-buffer-memory.js",
      "capabilities": {
        "whatItCanDo": [
          "Store complete conversation history in memory",
          "Maintain context across multiple interactions",
          "Enable personalized and contextual responses",
          "Preserve user information throughout sessions",
          "Support natural conversation flow"
        ],
        "bestUseCases": [
          "Customer support chatbots",
          "Personal assistant applications",
          "Educational tutoring systems",
          "Interactive storytelling",
          "Short to medium-length conversations"
        ],
        "limitations": [
          "Memory grows indefinitely without limits",
          "Can become expensive with long conversations",
          "No automatic cleanup of old information",
          "May hit token limits in very long sessions"
        ],
        "keyTakeaways": [
          "Simplest form of conversation memory in LangChain",
          "Perfect for maintaining context in conversations",
          "Essential for building conversational AI applications",
          "Easy to implement and understand",
          "Foundation for more advanced memory patterns"
        ]
      }
    },
    {
      "id": "buffer-window-memory",
      "title": "Buffer Window Memory",
      "description": "Keep only the last K interactions to limit memory size",
      "category": "memory",
      "codeSnippet": "import { BufferWindowMemory } from 'langchain/memory';\nimport { ConversationChain } from 'langchain/chains';\nimport { ChatOpenAI } from '@langchain/openai';\n\n// Initialize the model\nconst model = new ChatOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n    temperature: 0.7,\n    modelName: 'gpt-3.5-turbo'\n});\n\nconsole.log('ðŸ¤– Model Configuration:');\nconsole.log('   â€¢ Model: gpt-3.5-turbo');\nconsole.log('   â€¢ Temperature: 0.7');\n\n// Create buffer window memory with window size of 2\nconst windowSize = 2;\nconst memory = new BufferWindowMemory({\n    memoryKey: \"chat_history\",\n    returnMessages: true,\n    k: windowSize, // Keep only last 2 message pairs\n});\n\nconsole.log('ðŸ’¾ Buffer Window Memory Configuration:');\nconsole.log('   â€¢ Memory Key: chat_history');\nconsole.log('   â€¢ Window Size (k): 2 message pairs');\nconsole.log('   â€¢ Return Messages: true');\nconsole.log('   â€¢ Auto-cleanup: enabled');\n\n// Create conversation chain with window memory\nconst conversationChain = new ConversationChain({\n    llm: model,\n    memory: memory,\n    verbose: true,\n});\n\nconsole.log('ðŸ”— Conversation Chain Created with Window Memory');\n\n// Extended conversation to demonstrate window behavior\nconst conversationSteps = [\n    { input: \"My favorite color is blue.\", description: \"Personal preference #1\" },\n    { input: \"I work as a software engineer.\", description: \"Professional information\" },\n    { input: \"I have a pet cat named Whiskers.\", description: \"Personal information about pet\" },\n    { input: \"I enjoy hiking on weekends.\", description: \"Hobby information\" },\n    { input: \"My favorite programming language is Python.\", description: \"Technical preference\" },\n    { input: \"What do you remember about me?\", description: \"Memory test - should only remember recent interactions\" }\n];\n\nconsole.log('ðŸ—£ï¸ Starting Extended Conversation...');\nconsole.log(`ðŸ“ Window size: ${windowSize} message pairs`);\nconsole.log('ðŸ”„ Older messages will be automatically discarded');\n\nfor (let i = 0; i < conversationSteps.length; i++) {\n    const step = conversationSteps[i];\n    console.log(`ðŸ’¬ Step ${i + 1}: ${step.input}`);\n    \n    const response = await conversationChain.call({ input: step.input });\n    console.log('ðŸ¤– AI Response:', response.response);\n    \n    // Show current memory state\n    const memoryVariables = await memory.loadMemoryVariables({});\n    const messageCount = memoryVariables.chat_history.length;\n    const pairCount = Math.floor(messageCount / 2);\n    console.log(`ðŸªŸ Window state: ${pairCount}/${windowSize} pairs (${messageCount} messages)`);\n}",
      "demoFunction": "bufferWindowMemoryDemo",
      "demoFile": "src/demos/memory/buffer-window-memory.js",
      "capabilities": {
        "whatItCanDo": [
          "Maintain a sliding window of recent conversation history",
          "Automatically discard older interactions to control memory size",
          "Provide recent context while preventing memory overflow",
          "Balance context retention with resource efficiency",
          "Enable long-running conversations with bounded memory"
        ],
        "bestUseCases": [
          "Long-running customer service sessions",
          "Extended educational conversations",
          "Continuous monitoring and assistance systems",
          "Resource-constrained environments",
          "Applications requiring predictable memory usage"
        ],
        "limitations": [
          "Loses important early conversation context",
          "May forget crucial information from beyond the window",
          "Fixed window size may not suit all scenarios",
          "No intelligent selection of what to remember"
        ],
        "keyTakeaways": [
          "Excellent balance between context and efficiency",
          "Prevents memory from growing indefinitely",
          "Great for production systems with memory constraints",
          "Simple but effective memory management strategy",
          "Essential for scalable conversational applications"
        ]
      }
    },
    {
      "id": "conversation-summary-memory",
      "title": "Conversation Summary Memory",
      "description": "Summarize old conversations to maintain key information",
      "category": "memory",
      "codeSnippet": "import { ConversationSummaryMemory } from 'langchain/memory';\nimport { ConversationChain } from 'langchain/chains';\nimport { ChatOpenAI } from '@langchain/openai';\n\n// Initialize the model\nconst model = new ChatOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n    temperature: 0.7,\n    modelName: 'gpt-3.5-turbo'\n});\n\nconsole.log('ðŸ¤– Model Configuration:');\nconsole.log('   â€¢ Model: gpt-3.5-turbo');\nconsole.log('   â€¢ Temperature: 0.7');\n\n// Create conversation summary memory\nconst memory = new ConversationSummaryMemory({\n    memoryKey: \"chat_history\",\n    llm: model,\n    returnMessages: true,\n});\n\nconsole.log('ðŸ’¾ Summary Memory Configuration:');\nconsole.log('   â€¢ Memory Key: chat_history');\nconsole.log('   â€¢ LLM: gpt-3.5-turbo (for summarization)');\nconsole.log('   â€¢ Return Messages: true');\nconsole.log('   â€¢ Auto-summarization: enabled');\n\n// Create conversation chain with summary memory\nconst conversationChain = new ConversationChain({\n    llm: model,\n    memory: memory,\n    verbose: true,\n});\n\nconsole.log('ðŸ”— Conversation Chain Created with Summary Memory');\n\n// Simulate a longer conversation that would benefit from summarization\nconst longConversation = [\n    { input: \"I'm planning a trip to Japan next month.\", description: \"Initial travel planning\" },\n    { input: \"I'm particularly interested in visiting Tokyo and Kyoto.\", description: \"Specific destination preferences\" },\n    { input: \"I love Japanese cuisine, especially sushi and ramen.\", description: \"Food preferences\" },\n    { input: \"My budget is around $3000 for the entire trip.\", description: \"Budget constraints\" },\n    { input: \"I prefer cultural experiences over nightlife.\", description: \"Activity preferences\" },\n    { input: \"Can you give me some travel recommendations?\", description: \"Request for recommendations\" }\n];\n\nconsole.log('ðŸ—£ï¸ Starting Long Conversation...');\nconsole.log('ðŸ“ Conversation will be automatically summarized');\n\nfor (let i = 0; i < longConversation.length; i++) {\n    const step = longConversation[i];\n    console.log(`ðŸ’¬ Step ${i + 1}: ${step.input}`);\n    \n    const response = await conversationChain.call({ input: step.input });\n    console.log('ðŸ¤– AI Response:', response.response);\n    \n    // Show memory state (summary gets created automatically)\n    const memoryVariables = await memory.loadMemoryVariables({});\n    console.log('ðŸ“ Memory summary updated automatically');\n}\n\n// Show final summary\nconst finalMemory = await memory.loadMemoryVariables({});\nconsole.log('ðŸ“ Final Conversation Summary:', finalMemory.chat_history);",
      "demoFunction": "conversationSummaryMemoryDemo",
      "demoFile": "src/demos/memory/conversation-summary-memory.js",
      "capabilities": {
        "whatItCanDo": [
          "Automatically summarize older conversation parts to save space",
          "Preserve key information while reducing memory footprint",
          "Handle very long conversations efficiently",
          "Use AI to intelligently compress conversation history",
          "Maintain context continuity across extended sessions"
        ],
        "bestUseCases": [
          "Very long customer support sessions",
          "Extended therapy or counseling conversations",
          "Long-form educational interactions",
          "Complex project planning discussions",
          "Multi-session conversations spanning days or weeks"
        ],
        "limitations": [
          "Requires additional LLM calls for summarization",
          "May lose nuanced details in the summarization process",
          "Summary quality depends on the underlying LLM",
          "Increased latency due to summarization overhead"
        ],
        "keyTakeaways": [
          "Intelligent approach to managing long conversation memory",
          "Balances context preservation with efficiency",
          "Essential for applications with very long interactions",
          "Uses AI to make smart decisions about what to keep",
          "More sophisticated than simple window-based approaches"
        ]
      }
    },
    {
      "id": "custom-memory-lcel",
      "title": "Custom Memory with LCEL",
      "description": "Implement custom memory logic using LCEL patterns",
      "category": "memory",
      "codeSnippet": "// Simple in-memory storage for conversation history\nlet conversationHistory: ConversationTurn[] = [];\n\nconst addToMemory = (human: string, ai: string) => {\n    conversationHistory.push({ human, ai });\n    // Keep only last 5 interactions\n    if (conversationHistory.length > 5) {\n        conversationHistory = conversationHistory.slice(-5);\n    }\n};\n\nconst getMemoryContext = () => {\n    return conversationHistory\n        .map(turn => `Human: ${turn.human}\\nAI: ${turn.ai}`)\n        .join('\\n\\n');\n};\n\n// Create LCEL chain with custom memory\nconst chain = RunnableSequence.from([\n    {\n        history: () => getMemoryContext(),\n        input: (input: MemoryInput) => input.input,\n    },\n    prompt,\n    model,\n    new StringOutputParser(),\n]);",
      "demoFunction": "customMemoryLCELDemo",
      "demoFile": "src/demos/memory/custom-memory-lcel.js",
      "capabilities": {
        "whatItCanDo": [
          "Implement completely custom memory management logic",
          "Integrate memory seamlessly with LCEL chains",
          "Create domain-specific memory storage strategies",
          "Build flexible memory systems with custom rules",
          "Combine memory with other custom processing steps"
        ],
        "bestUseCases": [
          "Applications requiring specialized memory logic",
          "Integration with external databases or storage",
          "Custom business rules for information retention",
          "Performance-optimized memory implementations",
          "Complex memory patterns not covered by built-in types"
        ],
        "limitations": [
          "Requires manual implementation of memory logic",
          "No built-in persistence or recovery mechanisms",
          "Developer responsible for memory management edge cases",
          "More complex to implement and maintain"
        ],
        "keyTakeaways": [
          "Ultimate flexibility in memory management",
          "Great for specialized or complex requirements",
          "Demonstrates LCEL's extensibility",
          "Requires careful design and testing",
          "Foundation for building advanced memory systems"
        ]
      }
    },
    {
      "id": "memory-comparison",
      "title": "Memory Types Comparison",
      "description": "Compare different memory types and their use cases",
      "category": "memory",
      "codeSnippet": "// Memory Types Overview:\n\n// 1. BufferMemory: Stores all conversation history\n//    - Pros: Complete context, simple implementation\n//    - Cons: Can become very long, expensive\n//    - Use case: Short conversations\n\n// 2. BufferWindowMemory: Stores only last K interactions\n//    - Pros: Fixed memory size, prevents overflow\n//    - Cons: Loses older context\n//    - Use case: Long conversations, recent context focus\n\n// 3. ConversationSummaryMemory: Summarizes old conversations\n//    - Pros: Maintains key information, handles long conversations\n//    - Cons: May lose details, requires additional LLM calls\n//    - Use case: Very long conversations\n\n// 4. Custom Memory: Tailored to specific needs\n//    - Pros: Full control, domain-specific logic\n//    - Cons: More development effort\n//    - Use case: Specialized applications\n\n// Choosing the Right Memory Type:\n// - Short conversations: BufferMemory\n// - Long conversations: BufferWindowMemory\n// - Very long conversations: ConversationSummaryMemory\n// - Special requirements: Custom Memory",
      "demoFunction": "memoryComparisonDemo",
      "demoFile": "src/demos/memory/memory-comparison.js",
      "capabilities": {
        "whatItCanDo": [
          "Provide comprehensive overview of all memory types",
          "Compare pros and cons of different memory strategies",
          "Guide selection of appropriate memory type for use cases",
          "Demonstrate practical differences between memory approaches",
          "Help optimize memory usage for specific applications"
        ],
        "bestUseCases": [
          "Learning about LangChain memory options",
          "Architecture planning for conversational AI",
          "Performance optimization decisions",
          "Educational demonstrations",
          "Memory strategy evaluation"
        ],
        "limitations": [
          "Theoretical comparison rather than hands-on implementation",
          "May not cover all edge cases or hybrid approaches",
          "Requires understanding of application requirements",
          "General guidance may not fit all specific scenarios"
        ],
        "keyTakeaways": [
          "Different memory types serve different purposes",
          "Choice depends on conversation length and requirements",
          "Trade-offs exist between context retention and efficiency",
          "Understanding options is key to good architecture",
          "Foundation for making informed memory decisions"
        ]
      }
    }
  ],
  "agents": [
    {
      "id": "basic-agent",
      "title": "Basic ReAct Agent",
      "description": "Simple agent that can reason and act with built-in tools",
      "category": "agents",
      "codeSnippet": "import { AgentExecutor, createReactAgent } from 'langchain/agents';\nimport { pull } from 'langchain/hub';\nimport { ChatOpenAI } from '@langchain/openai';\nimport { Calculator } from 'langchain/tools/calculator';\nimport { SerpAPI } from 'langchain/tools/serpapi';\n\n// Initialize the LLM\nconst llm = new ChatOpenAI({\n  temperature: 0,\n  modelName: 'gpt-3.5-turbo',\n});\n\n// Define available tools\nconst tools = [\n  new Calculator(),\n  new SerpAPI(process.env.SERPAPI_API_KEY),\n];\n\n// Get the prompt template from LangChain Hub\nconst prompt = await pull('hwchase17/react');\n\n// Create the ReAct agent\nconst agent = await createReactAgent({\n  llm,\n  tools,\n  prompt,\n});\n\n// Create agent executor\nconst agentExecutor = new AgentExecutor({\n  agent,\n  tools,\n  verbose: true,\n});\n\n// Execute agent with a complex query\nconst result = await agentExecutor.invoke({\n  input: 'What is the square root of 144 multiplied by 7?',\n});\n\nconsole.log(result.output);",
      "demoFunction": "basicReactAgentDemo",
      "demoFile": "src/demos/agents/basic-react-agent.js",
      "capabilities": {
        "whatItCanDo": [
          "Break down complex problems into steps",
          "Use multiple tools in sequence",
          "Self-correct when tools return unexpected results",
          "Reason about which tool to use for each subtask",
          "Combine information from multiple sources"
        ],
        "bestUseCases": [
          "Mathematical problem solving",
          "Information research and synthesis",
          "Multi-step task automation",
          "Data analysis and reporting",
          "Customer support with tool access"
        ],
        "limitations": [
          "Limited by available tools",
          "Can make reasoning errors",
          "Token usage grows with complexity",
          "May get stuck in reasoning loops"
        ],
        "keyTakeaways": [
          "ReAct agents combine reasoning with tool usage",
          "They can solve complex multi-step problems",
          "Tool selection and usage is automatic",
          "Verbose mode helps understand agent reasoning",
          "Great foundation for building intelligent assistants"
        ]
      }
    },
    {
      "id": "custom-tools",
      "title": "Custom Tools Creation",
      "description": "Creating and using custom tools with agents",
      "category": "agents",
      "codeSnippet": "import { Tool } from 'langchain/tools';\nimport { z } from 'zod';\n\n// Define a custom weather tool\nclass WeatherTool extends Tool {\n  name = 'weather';\n  description = 'Get current weather for a city. Input should be a city name.';\n\n  schema = z.object({\n    city: z.string().describe('The city name to get weather for'),\n  });\n\n  async _call(input: string): Promise<string> {\n    // In a real implementation, you'd call a weather API\n    const weatherData = {\n      'New York': 'Sunny, 75Â°F',\n      'London': 'Cloudy, 60Â°F',\n      'Tokyo': 'Rainy, 68Â°F',\n    };\n    \n    return weatherData[input] || `Weather data not available for ${input}`;\n  }\n}\n\n// Define a custom database query tool\nclass DatabaseTool extends Tool {\n  name = 'database_query';\n  description = 'Query user database. Input should be a SQL-like query.';\n\n  async _call(query: string): Promise<string> {\n    // Mock database response\n    const mockResults = [\n      { id: 1, name: 'John Doe', age: 30 },\n      { id: 2, name: 'Jane Smith', age: 25 },\n    ];\n    \n    return JSON.stringify(mockResults, null, 2);\n  }\n}\n\n// Use custom tools with agent\nconst customTools = [\n  new WeatherTool(),\n  new DatabaseTool(),\n  new Calculator(),\n];\n\nconst agentWithCustomTools = new AgentExecutor({\n  agent: await createReactAgent({ llm, tools: customTools, prompt }),\n  tools: customTools,\n});",
      "demoFunction": "customToolsDemo",
      "demoFile": "src/demos/agents/custom-tools.js",
      "capabilities": {
        "whatItCanDo": [
          "Create custom tools with specific business logic",
          "Define tool schemas for input validation",
          "Integrate external APIs and services as tools",
          "Build domain-specific functionality for agents",
          "Combine custom tools with built-in LangChain tools"
        ],
        "bestUseCases": [
          "Integrating proprietary business systems",
          "Building domain-specific AI assistants",
          "Creating specialized data access tools",
          "Extending agent capabilities with custom APIs",
          "Building enterprise-specific automation tools"
        ],
        "limitations": [
          "Requires manual implementation of tool logic",
          "Developer responsible for error handling and validation",
          "No built-in caching or optimization",
          "Tool quality depends on implementation"
        ],
        "keyTakeaways": [
          "Custom tools unlock unlimited agent capabilities",
          "Essential for building production AI applications",
          "Proper schema definition improves tool usage",
          "Foundation for domain-specific AI solutions",
          "Bridges AI agents with existing business systems"
        ]
      }
    },
    {
      "id": "function-calling-agent",
      "title": "Function Calling Agent",
      "description": "Agent using OpenAI function calling for structured tool usage",
      "category": "agents",
      "codeSnippet": "import { createOpenAIFunctionsAgent, AgentExecutor } from 'langchain/agents';\nimport { ChatOpenAI } from '@langchain/openai';\nimport { DynamicTool } from 'langchain/tools';\n\n// Create function-calling compatible tools\nconst emailTool = new DynamicTool({\n  name: 'send_email',\n  description: 'Send an email to a recipient',\n  func: async ({ to, subject, body }) => {\n    console.log(`Sending email to: ${to}`);\n    console.log(`Subject: ${subject}`);\n    console.log(`Body: ${body}`);\n    return `Email sent successfully to ${to}`;\n  },\n  schema: z.object({\n    to: z.string().describe('Email recipient'),\n    subject: z.string().describe('Email subject'),\n    body: z.string().describe('Email body'),\n  }),\n});\n\nconst calendarTool = new DynamicTool({\n  name: 'schedule_meeting',\n  description: 'Schedule a meeting in the calendar',\n  func: async ({ title, date, duration }) => {\n    console.log(`Scheduling: ${title} on ${date} for ${duration} minutes`);\n    return `Meeting '${title}' scheduled for ${date}`;\n  },\n  schema: z.object({\n    title: z.string().describe('Meeting title'),\n    date: z.string().describe('Meeting date (YYYY-MM-DD)'),\n    duration: z.number().describe('Duration in minutes'),\n  }),\n});\n\nconst tools = [emailTool, calendarTool];\n\n// Create function calling agent\nconst llm = new ChatOpenAI({\n  modelName: 'gpt-3.5-turbo-0613',\n  temperature: 0,\n});\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  ['system', 'You are a helpful assistant that can send emails and schedule meetings.'],\n  ['human', '{input}'],\n  ['placeholder', '{agent_scratchpad}'],\n]);\n\nconst agent = await createOpenAIFunctionsAgent({\n  llm,\n  tools,\n  prompt,\n});\n\nconst agentExecutor = new AgentExecutor({\n  agent,\n  tools,\n});\n\nconst result = await agentExecutor.invoke({\n  input: 'Send an email to john@example.com about our meeting tomorrow at 2 PM, then schedule the meeting in the calendar.',\n});",
      "demoFunction": "functionCallingAgentDemo",
      "demoFile": "src/demos/agents/function-calling-agent.js",
      "capabilities": {
        "whatItCanDo": [
          "Use OpenAI's function calling for structured tool interactions",
          "Automatically parse and validate tool parameters",
          "Handle complex multi-parameter tool calls",
          "Provide better error handling for malformed tool usage",
          "Enable more reliable and structured agent behavior"
        ],
        "bestUseCases": [
          "Production applications requiring reliable tool usage",
          "Complex business process automation",
          "Structured data manipulation tasks",
          "API integrations with strict parameter requirements",
          "Enterprise applications with validation needs"
        ],
        "limitations": [
          "Requires OpenAI models with function calling support",
          "Limited to models that support function calling",
          "May have higher latency due to function parsing",
          "Dependent on OpenAI's function calling implementation"
        ],
        "keyTakeaways": [
          "Function calling provides more reliable tool usage",
          "Better for production applications than basic ReAct",
          "Structured approach reduces tool usage errors",
          "Essential for complex business applications",
          "Represents the future of agent-tool interactions"
        ]
      }
    },
    {
      "id": "multi-agent-system",
      "title": "Multi-Agent System",
      "description": "Coordinating multiple specialized agents for complex tasks",
      "category": "agents",
      "codeSnippet": "// Define specialized agents\nclass ResearchAgent {\n  constructor(llm, tools) {\n    this.llm = llm;\n    this.tools = tools.filter(tool => \n      ['serpapi', 'wikipedia'].includes(tool.name)\n    );\n  }\n\n  async research(topic) {\n    const prompt = `Research the topic: ${topic}. Provide key facts and insights.`;\n    // Agent logic here\n    return `Research completed on ${topic}`;\n  }\n}\n\nclass WritingAgent {\n  constructor(llm) {\n    this.llm = llm;\n  }\n\n  async write(research, style = 'professional') {\n    const prompt = `Based on this research: ${research}\n\nWrite a ${style} article.`;\n    // Agent logic here\n    return `Article written in ${style} style`;\n  }\n}\n\nclass ReviewAgent {\n  constructor(llm) {\n    this.llm = llm;\n  }\n\n  async review(content) {\n    const prompt = `Review and improve this content: ${content}`;\n    // Agent logic here\n    return `Content reviewed and improved`;\n  }\n}\n\n// Orchestrator to coordinate agents\nclass MultiAgentOrchestrator {\n  constructor() {\n    this.researchAgent = new ResearchAgent(llm, tools);\n    this.writingAgent = new WritingAgent(llm);\n    this.reviewAgent = new ReviewAgent(llm);\n  }\n\n  async processTask(task) {\n    console.log('Starting multi-agent workflow...');\n    \n    // Step 1: Research\n    const research = await this.researchAgent.research(task.topic);\n    console.log('Research phase completed');\n    \n    // Step 2: Writing\n    const article = await this.writingAgent.write(research, task.style);\n    console.log('Writing phase completed');\n    \n    // Step 3: Review\n    const finalContent = await this.reviewAgent.review(article);\n    console.log('Review phase completed');\n    \n    return finalContent;\n  }\n}\n\n// Usage\nconst orchestrator = new MultiAgentOrchestrator();\nconst result = await orchestrator.processTask({\n  topic: 'Artificial Intelligence in Healthcare',\n  style: 'technical'\n});",
      "demoFunction": "multiAgentSystemDemo",
      "demoFile": "src/demos/agents/multi-agent-system.js",
      "capabilities": {
        "whatItCanDo": [
          "Coordinate multiple specialized agents for complex workflows",
          "Break down complex tasks into specialized sub-tasks",
          "Enable parallel processing across different agent types",
          "Orchestrate sequential and parallel agent interactions",
          "Build sophisticated AI systems with division of labor"
        ],
        "bestUseCases": [
          "Complex content creation workflows",
          "Multi-step business process automation",
          "Research and analysis pipelines",
          "Quality assurance and review systems",
          "Enterprise-scale AI automation"
        ],
        "limitations": [
          "Increased complexity in coordination and error handling",
          "Higher computational costs due to multiple agents",
          "Potential for coordination failures between agents",
          "Requires careful design of agent interactions"
        ],
        "keyTakeaways": [
          "Multi-agent systems enable sophisticated AI workflows",
          "Specialization improves individual agent performance",
          "Orchestration is key to successful multi-agent systems",
          "Essential for complex enterprise applications",
          "Represents advanced AI system architecture"
        ]
      }
    },
    {
      "id": "agent-with-memory",
      "title": "Agent with Memory",
      "description": "Persistent agent that remembers previous interactions",
      "category": "agents",
      "codeSnippet": "import { BufferMemory } from 'langchain/memory';\nimport { ConversationChain } from 'langchain/chains';\n\n// Create agent with persistent memory\nclass MemoryAgent {\n  constructor(llm, tools) {\n    this.memory = new BufferMemory({\n      memoryKey: 'chat_history',\n      returnMessages: true,\n    });\n    \n    this.agent = new AgentExecutor({\n      agent: await createReactAgent({ llm, tools, prompt }),\n      tools: tools,\n      memory: this.memory,\n      verbose: true,\n    });\n  }\n\n  async chat(input) {\n    const result = await this.agent.invoke({\n      input,\n      chat_history: await this.memory.chatHistory.getMessages(),\n    });\n    \n    // Save interaction to memory\n    await this.memory.saveContext(\n      { input },\n      { output: result.output }\n    );\n    \n    return result.output;\n  }\n\n  async getMemory() {\n    return await this.memory.chatHistory.getMessages();\n  }\n\n  clearMemory() {\n    this.memory.clear();\n  }\n}\n\n// Usage example\nconst memoryAgent = new MemoryAgent(llm, tools);\n\n// First interaction\nconst response1 = await memoryAgent.chat(\n  'My name is Alice and I work as a software engineer.'\n);\n\n// Second interaction - agent remembers previous context\nconst response2 = await memoryAgent.chat(\n  'What did I tell you about my profession?'\n);\n\n// Check memory\nconst memory = await memoryAgent.getMemory();\nconsole.log('Agent memory:', memory);",
      "demoFunction": "agentWithMemoryDemo",
      "demoFile": "src/demos/agents/agent-with-memory.js",
      "capabilities": {
        "whatItCanDo": [
          "Maintain conversation context across multiple interactions",
          "Remember user preferences and previous decisions",
          "Build upon previous conversation history",
          "Provide personalized responses based on past interactions",
          "Create continuous, contextual agent experiences"
        ],
        "bestUseCases": [
          "Personal assistant applications",
          "Long-term customer support interactions",
          "Educational tutoring systems",
          "Personalized recommendation systems",
          "Continuous workflow automation"
        ],
        "limitations": [
          "Memory management complexity increases over time",
          "Potential for memory to become outdated or irrelevant",
          "Higher computational overhead due to memory processing",
          "Privacy concerns with persistent memory storage"
        ],
        "keyTakeaways": [
          "Memory transforms agents from stateless to stateful systems",
          "Essential for building truly conversational AI",
          "Enables personalization and context continuity",
          "Requires careful memory management strategies",
          "Foundation for advanced AI assistant applications"
        ]
      }
    },
    {
      "id": "tool-integration",
      "title": "Advanced Tool Integration",
      "description": "Integrating external APIs and services as agent tools",
      "category": "agents",
      "codeSnippet": "import { DynamicTool } from 'langchain/tools';\nimport axios from 'axios';\n\n// GitHub API Tool\nclass GitHubTool extends Tool {\n  name = 'github_search';\n  description = 'Search GitHub repositories. Input should be a search query.';\n\n  async _call(query: string): Promise<string> {\n    try {\n      const response = await axios.get(\n        `https://api.github.com/search/repositories?q=${encodeURIComponent(query)}&sort=stars&order=desc&per_page=5`\n      );\n      \n      const repos = response.data.items.map(repo => ({\n        name: repo.full_name,\n        description: repo.description,\n        stars: repo.stargazers_count,\n        url: repo.html_url,\n      }));\n      \n      return JSON.stringify(repos, null, 2);\n    } catch (error) {\n      return `Error searching GitHub: ${error.message}`;\n    }\n  }\n}\n\n// News API Tool\nclass NewsTool extends Tool {\n  name = 'get_news';\n  description = 'Get latest news headlines. Input should be a topic or category.';\n\n  async _call(topic: string): Promise<string> {\n    try {\n      // Mock news API response\n      const mockNews = [\n        {\n          title: `Breaking: ${topic} developments announced`,\n          source: 'Tech News',\n          publishedAt: new Date().toISOString(),\n        },\n        {\n          title: `Analysis: Impact of ${topic} on industry`,\n          source: 'Industry Weekly',\n          publishedAt: new Date().toISOString(),\n        },\n      ];\n      \n      return JSON.stringify(mockNews, null, 2);\n    } catch (error) {\n      return `Error fetching news: ${error.message}`;\n    }\n  }\n}\n\n// File System Tool\nclass FileSystemTool extends Tool {\n  name = 'file_operations';\n  description = 'Perform file operations like read, write, list. Input format: \"operation:path\".';\n\n  async _call(input: string): Promise<string> {\n    const [operation, path] = input.split(':');\n    \n    switch (operation) {\n      case 'list':\n        // Mock directory listing\n        return JSON.stringify(['file1.txt', 'file2.js', 'folder1/'], null, 2);\n      case 'read':\n        return `Content of ${path}: Mock file content here...`;\n      case 'write':\n        return `Successfully wrote to ${path}`;\n      default:\n        return `Unknown operation: ${operation}`;\n    }\n  }\n}\n\n// Create agent with integrated tools\nconst advancedTools = [\n  new GitHubTool(),\n  new NewsTool(),\n  new FileSystemTool(),\n  new Calculator(),\n];\n\nconst advancedAgent = new AgentExecutor({\n  agent: await createReactAgent({ llm, tools: advancedTools, prompt }),\n  tools: advancedTools,\n  verbose: true,\n});",
      "demoFunction": "advancedToolIntegrationDemo",
      "demoFile": "src/demos/agents/advanced-tool-integration.js",
      "capabilities": {
        "whatItCanDo": [
          "Integrate external APIs and services as agent tools",
          "Create sophisticated tool ecosystems for agents",
          "Handle complex API authentication and error handling",
          "Build bridges between AI agents and existing systems",
          "Enable agents to interact with real-world services"
        ],
        "bestUseCases": [
          "Enterprise system integration",
          "API-driven business automation",
          "Real-time data access and manipulation",
          "External service orchestration",
          "Building comprehensive AI-powered platforms"
        ],
        "limitations": [
          "Dependent on external API reliability and availability",
          "Requires proper error handling and fallback strategies",
          "API rate limits and authentication complexity",
          "Security considerations for API key management"
        ],
        "keyTakeaways": [
          "Advanced tool integration unlocks real-world AI applications",
          "Essential for production-ready AI systems",
          "Proper error handling is critical for reliability",
          "Security and authentication must be carefully managed",
          "Foundation for building comprehensive AI platforms"
        ]
      }
    }
  ],
  "rag": [
    {
      "id": "basic-document-loading",
      "title": "Basic Document Loading",
      "description": "Load and process various document formats (PDF, TXT, DOCX)",
      "category": "rag",
      "codeSnippet": "import { TextLoader } from 'langchain/document_loaders/fs/text';\nimport { CSVLoader } from 'langchain/document_loaders/fs/csv';\nimport { JSONLoader } from 'langchain/document_loaders/fs/json';\nimport { Document } from '@langchain/core/documents';\nimport { writeFileSync, mkdirSync, existsSync } from 'fs';\nimport { join, dirname } from 'path';\n\nconsole.log('ðŸ“„ Document Loading Overview:');\nconsole.log('   â€¢ Load various document formats (TXT, CSV, JSON)');\nconsole.log('   â€¢ Extract text content and metadata');\nconsole.log('   â€¢ Prepare documents for RAG processing');\n\n// Create sample documents directory\nconst sampleDir = join(__dirname, 'sample_documents');\nif (!existsSync(sampleDir)) {\n    mkdirSync(sampleDir, { recursive: true });\n}\n\n// Create and load text document\nconst textContent = `# Introduction to Artificial Intelligence\\n\\nArtificial Intelligence (AI) is a branch of computer science...`;\nconst textPath = join(sampleDir, 'ai_introduction.txt');\nwriteFileSync(textPath, textContent);\n\nconst textLoader = new TextLoader(textPath);\nconst textDocs = await textLoader.load();\nconsole.log(`ðŸ“ Loaded text document: ${textDocs[0].pageContent.substring(0, 100)}...`);\n\n// Create and load CSV document\nconst csvContent = `name,age,department,salary\\nJohn Doe,30,Engineering,75000\\nJane Smith,28,Marketing,65000`;\nconst csvPath = join(sampleDir, 'employee_data.csv');\nwriteFileSync(csvPath, csvContent);\n\nconst csvLoader = new CSVLoader(csvPath);\nconst csvDocs = await csvLoader.load();\nconsole.log(`ðŸ“Š Loaded CSV with ${csvDocs.length} rows`);\n\n// Process document metadata\ntextDocs.forEach((doc, index) => {\n    console.log(`Document ${index}:`, {\n        source: doc.metadata.source,\n        contentLength: doc.pageContent.length,\n        type: 'text'\n    });\n});",
      "demoFunction": "basicDocumentLoadingDemo",
      "demoFile": "src/demos/rag/basic-document-loading.js",
      "capabilities": {
        "whatItCanDo": [
          "Load and parse multiple document formats (PDF, TXT, DOCX, CSV)",
          "Extract text content while preserving metadata",
          "Handle batch document processing",
          "Maintain document structure and formatting information",
          "Support various encoding and file format specifications"
        ],
        "bestUseCases": [
          "Building document knowledge bases",
          "Content migration and digitization",
          "Document analysis and processing pipelines",
          "Research paper and report processing",
          "Enterprise document management systems"
        ],
        "limitations": [
          "Limited by document format support in loaders",
          "May lose formatting and visual elements",
          "Large files can consume significant memory",
          "OCR may be needed for scanned documents"
        ],
        "keyTakeaways": [
          "Foundation step for any RAG system",
          "Proper document loading affects downstream quality",
          "Metadata preservation is crucial for traceability",
          "Different formats require different loading strategies",
          "Essential for building comprehensive knowledge systems"
        ]
      }
    },
    {
      "id": "text-splitting",
      "title": "Text Splitting & Chunking",
      "description": "Split large documents into manageable chunks for processing",
      "category": "rag",
      "codeSnippet": "import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';\nimport { TokenTextSplitter } from 'langchain/text_splitter';\nimport { MarkdownTextSplitter } from 'langchain/text_splitter';\n\n// Basic recursive character splitting\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n  separators: ['\\n\\n', '\\n', ' ', ''],\n});\n\nconst docs = await textSplitter.createDocuments([longText]);\nconsole.log(`Split into ${docs.length} chunks`);\n\n// Token-based splitting (useful for LLM context limits)\nconst tokenSplitter = new TokenTextSplitter({\n  chunkSize: 512,\n  chunkOverlap: 50,\n});\n\nconst tokenDocs = await tokenSplitter.splitDocuments(docs);\nconsole.log(`Token-based splitting: ${tokenDocs.length} chunks`);\n\n// Markdown-aware splitting\nconst markdownSplitter = new MarkdownTextSplitter({\n  chunkSize: 800,\n  chunkOverlap: 100,\n});\n\nconst markdownDocs = await markdownSplitter.createDocuments([markdownText]);\n\n// Custom splitting with metadata preservation\nconst customSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 500,\n  chunkOverlap: 50,\n  keepSeparator: true,\n});\n\nconst chunkedDocs = await customSplitter.splitDocuments(originalDocs);\nchunkedDocs.forEach((chunk, index) => {\n  console.log(`Chunk ${index}:`, {\n    length: chunk.pageContent.length,\n    source: chunk.metadata.source,\n    chunkIndex: index\n  });\n});",
      "demoFunction": "textSplittingDemo",
      "demoFile": "src/demos/rag/text-splitting.js",
      "capabilities": {
        "whatItCanDo": [
          "Split large documents into manageable chunks",
          "Preserve context with configurable overlap between chunks",
          "Handle different content types (text, markdown, code)",
          "Optimize chunks for LLM context windows",
          "Maintain semantic coherence within chunks"
        ],
        "bestUseCases": [
          "Preparing documents for vector storage",
          "Optimizing content for LLM processing",
          "Creating searchable document segments",
          "Managing large document collections",
          "Building efficient RAG systems"
        ],
        "limitations": [
          "May break semantic coherence across chunk boundaries",
          "Fixed chunk sizes may not respect natural boundaries",
          "Overlap can create redundancy in vector stores",
          "Requires tuning for optimal performance"
        ],
        "keyTakeaways": [
          "Critical preprocessing step for RAG systems",
          "Chunk size and overlap significantly impact retrieval quality",
          "Different content types need different splitting strategies",
          "Balance between chunk size and semantic coherence",
          "Foundation for effective document retrieval"
        ]
      }
    },
    {
      "id": "vector-embeddings",
      "title": "Vector Embeddings & Storage",
      "description": "Create embeddings and store documents in vector databases",
      "category": "rag",
      "codeSnippet": "import { OpenAIEmbeddings } from '@langchain/openai';\nimport { MemoryVectorStore } from 'langchain/vectorstores/memory';\nimport { Chroma } from 'langchain/vectorstores/chroma';\nimport { Pinecone } from 'langchain/vectorstores/pinecone';\n\n// Initialize embeddings model\nconst embeddings = new OpenAIEmbeddings({\n  apiKey: process.env.OPENAI_API_KEY,\n  modelName: 'text-embedding-ada-002',\n});\n\n// Create in-memory vector store (for development)\nconst memoryVectorStore = await MemoryVectorStore.fromDocuments(\n  documents,\n  embeddings\n);\n\nconsole.log(`Created memory vector store with ${documents.length} documents`);\n\n// Similarity search\nconst query = 'What is machine learning?';\nconst similarDocs = await memoryVectorStore.similaritySearch(query, 4);\n\nconsole.log(`Found ${similarDocs.length} similar documents:`);\nsimilarDocs.forEach((doc, index) => {\n  console.log(`${index + 1}. ${doc.pageContent.substring(0, 100)}...`);\n});\n\n// Similarity search with scores\nconst docsWithScores = await memoryVectorStore.similaritySearchWithScore(query, 3);\ndocsWithScores.forEach(([doc, score], index) => {\n  console.log(`${index + 1}. Score: ${score.toFixed(3)} - ${doc.pageContent.substring(0, 80)}...`);\n});\n\n// Advanced filtering\nconst filteredDocs = await memoryVectorStore.similaritySearch(\n  query,\n  2,\n  { source: 'specific-document.pdf' } // Filter by metadata\n);\n\n// Persistent vector store (Chroma example)\nconst chromaStore = await Chroma.fromDocuments(\n  documents,\n  embeddings,\n  {\n    collectionName: 'my-collection',\n    url: 'http://localhost:8000',\n  }\n);",
      "demoFunction": "vectorEmbeddingsDemo",
      "demoFile": "src/demos/rag/vector-embeddings.js",
      "capabilities": {
        "whatItCanDo": [
          "Convert text documents into high-dimensional vector representations",
          "Store and index vectors for fast similarity search",
          "Perform semantic search across document collections",
          "Support multiple vector database backends",
          "Enable similarity scoring and ranking of documents"
        ],
        "bestUseCases": [
          "Semantic search applications",
          "Document similarity and clustering",
          "Building knowledge retrieval systems",
          "Content recommendation engines",
          "Large-scale document analysis"
        ],
        "limitations": [
          "Embedding quality depends on the model used",
          "High-dimensional vectors require significant storage",
          "Vector database setup and maintenance complexity",
          "Computational cost for large document collections"
        ],
        "keyTakeaways": [
          "Core technology enabling semantic search",
          "Vector quality directly impacts retrieval performance",
          "Choice of embedding model is crucial",
          "Scalable solution for large document collections",
          "Foundation for modern AI-powered search systems"
        ]
      }
    },
    {
      "id": "basic-rag-chain",
      "title": "Basic RAG Chain",
      "description": "Build a simple retrieval-augmented generation system",
      "category": "rag",
      "codeSnippet": "import { ChatOpenAI, OpenAIEmbeddings } from '@langchain/openai';\nimport { MemoryVectorStore } from 'langchain/vectorstores/memory';\nimport { Document } from '@langchain/core/documents';\nimport { ChatPromptTemplate } from '@langchain/core/prompts';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\nimport { RunnableSequence, RunnablePassthrough } from '@langchain/core/runnables';\nimport { formatDocumentsAsString } from 'langchain/util/document';\nimport { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';\n\n// Initialize components\nconst llm = new ChatOpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  modelName: 'gpt-3.5-turbo',\n  temperature: 0.7,\n  maxTokens: 500,\n});\n\nconst embeddings = new OpenAIEmbeddings({\n  apiKey: process.env.OPENAI_API_KEY,\n  modelName: 'text-embedding-ada-002',\n});\n\n// Create knowledge base and split into chunks\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n  separators: ['\\n## ', '\\n### ', '\\n\\n', '\\n', ' ', ''],\n});\n\nconst docs = await textSplitter.createDocuments([knowledgeBase], [{\n  source: 'langchain_documentation',\n  type: 'documentation'\n}]);\n\n// Create vector store from documents\nconst vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);\n\n// Create retriever\nconst retriever = vectorStore.asRetriever({\n  k: 4, // Retrieve top 4 most similar documents\n  searchType: 'similarity',\n});\n\n// Create RAG prompt template\nconst ragPromptTemplate = ChatPromptTemplate.fromTemplate(`\nYou are a helpful assistant that answers questions based on the provided context.\n\nContext:\n{context}\n\nQuestion: {question}\n\nInstructions:\n- Answer the question based primarily on the provided context\n- If the context doesn't contain enough information, say so clearly\n- Provide specific details from the context when possible\n- Be concise but comprehensive\n\nAnswer:`);\n\n// Build the RAG chain using LCEL\nconst ragChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocumentsAsString),\n    question: new RunnablePassthrough(),\n  },\n  ragPromptTemplate,\n  llm,\n  new StringOutputParser(),\n]);\n\n// Test the RAG chain\nconst question = \"What is LangChain and what are its main purposes?\";\nconst response = await ragChain.invoke(question);\n\nconsole.log('Question:', question);\nconsole.log('Answer:', response);",
      "demoFunction": "basicRagChainDemo",
      "demoFile": "src/demos/rag/basic-rag-chain.js",
      "capabilities": {
        "whatItCanDo": [
          "Answer questions using relevant document context",
          "Retrieve and rank documents by similarity",
          "Combine retrieved information with LLM reasoning",
          "Provide source citations for answers",
          "Handle domain-specific knowledge queries"
        ],
        "bestUseCases": [
          "Document Q&A systems",
          "Knowledge base chatbots",
          "Research assistance tools",
          "Technical documentation helpers",
          "Customer support with knowledge retrieval"
        ],
        "limitations": [
          "Quality depends on document chunking strategy",
          "May miss relevant information across chunks",
          "Retrieval accuracy affects answer quality",
          "Limited by vector embedding capabilities"
        ],
        "keyTakeaways": [
          "Foundation of modern AI knowledge systems",
          "Combines the best of search and generation",
          "Essential for building domain-specific AI assistants",
          "Requires good document preprocessing",
          "Powerful alternative to fine-tuning for knowledge tasks"
        ]
      }
    },
    {
      "id": "advanced-rag-lcel",
      "title": "Advanced RAG with LCEL",
      "description": "Build sophisticated RAG pipelines using LangChain Expression Language",
      "category": "rag",
      "codeSnippet": "import { RunnableSequence, RunnableMap } from '@langchain/core/runnables';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\nimport { ChatPromptTemplate } from '@langchain/core/prompts';\n\n// Advanced RAG prompt with context formatting\nconst advancedRagPrompt = ChatPromptTemplate.fromMessages([\n  ['system', 'You are a helpful assistant that answers questions based on the provided context. Always cite your sources.'],\n  ['human', `Context information:\n{context}\n\nQuestion: {question}\n\nProvide a comprehensive answer based on the context above. If the context doesn't contain enough information, say so explicitly.`]\n]);\n\n// Context formatting function\nconst formatDocs = (docs) => {\n  return docs.map((doc, index) => \n    `[Source ${index + 1}: ${doc.metadata.source}]\n${doc.pageContent}`\n  ).join('\n\n');\n};\n\n// Advanced RAG chain with LCEL\nconst advancedRagChain = RunnableSequence.from([\n  {\n    context: (input) => retriever.getRelevantDocuments(input.question).then(formatDocs),\n    question: (input) => input.question,\n  },\n  advancedRagPrompt,\n  llm,\n  new StringOutputParser(),\n]);\n\n// Multi-query RAG for better retrieval\nconst multiQueryRag = RunnableSequence.from([\n  // Generate multiple query variations\n  {\n    queries: RunnableSequence.from([\n      ChatPromptTemplate.fromTemplate(\n        'Generate 3 different versions of this question for better document retrieval:\\n{question}'\n      ),\n      llm,\n      new StringOutputParser(),\n      (output) => output.split('\\n').filter(q => q.trim()),\n    ]),\n    originalQuestion: (input) => input.question,\n  },\n  // Retrieve documents for all queries\n  {\n    context: async (input) => {\n      const allDocs = [];\n      for (const query of input.queries) {\n        const docs = await retriever.getRelevantDocuments(query);\n        allDocs.push(...docs);\n      }\n      // Remove duplicates and format\n      const uniqueDocs = Array.from(new Set(allDocs.map(d => d.pageContent)))\n        .map(content => allDocs.find(d => d.pageContent === content));\n      return formatDocs(uniqueDocs.slice(0, 6));\n    },\n    question: (input) => input.originalQuestion,\n  },\n  advancedRagPrompt,\n  llm,\n  new StringOutputParser(),\n]);\n\n// Usage\nconst complexQuestion = 'How do modern AI systems handle uncertainty and what are the implications?';\nconst result = await advancedRagChain.invoke({ question: complexQuestion });\nconsole.log('Advanced RAG Response:', result);",
      "demoFunction": "advancedRagLcelDemo",
      "demoFile": "src/demos/rag/advanced-rag-lcel.js",
      "capabilities": {
        "whatItCanDo": [
          "Build sophisticated RAG pipelines using LCEL",
          "Implement multi-query retrieval for better coverage",
          "Create custom context formatting and processing",
          "Chain complex retrieval and generation operations",
          "Enable advanced prompt engineering with dynamic context"
        ],
        "bestUseCases": [
          "Complex knowledge systems requiring sophisticated retrieval",
          "Research applications needing comprehensive context",
          "Enterprise RAG systems with custom processing needs",
          "Advanced question-answering with multiple retrieval strategies",
          "Building production-ready RAG applications"
        ],
        "limitations": [
          "Increased complexity in pipeline design and debugging",
          "Higher computational costs due to multiple queries",
          "Requires deep understanding of LCEL patterns",
          "More complex error handling and monitoring"
        ],
        "keyTakeaways": [
          "LCEL enables sophisticated RAG architectures",
          "Multi-query approaches improve retrieval quality",
          "Custom processing pipelines offer maximum flexibility",
          "Essential for building advanced production RAG systems",
          "Represents the cutting edge of RAG implementation"
        ]
      }
    },
    {
      "id": "conversational-rag",
      "title": "Conversational RAG",
      "description": "RAG system that maintains conversation history and context",
      "category": "rag",
      "codeSnippet": "import { ConversationalRetrievalQAChain } from 'langchain/chains';\nimport { BufferMemory } from 'langchain/memory';\n\n// Create conversational memory\nconst memory = new BufferMemory({\n  memoryKey: 'chat_history',\n  returnMessages: true,\n});\n\n// Conversational RAG chain\nconst conversationalRagChain = ConversationalRetrievalQAChain.fromLLM(\n  llm,\n  retriever,\n  {\n    memory,\n    returnSourceDocuments: true,\n    verbose: true,\n  }\n);\n\n// Custom conversational RAG with LCEL\nconst conversationalPrompt = ChatPromptTemplate.fromMessages([\n  ['system', 'You are a helpful assistant. Use the context and chat history to provide accurate answers.'],\n  ['human', `Chat History:\n{chat_history}\n\nContext:\n{context}\n\nCurrent Question: {question}`]\n]);\n\nclass ConversationalRAG {\n  constructor(llm, retriever) {\n    this.llm = llm;\n    this.retriever = retriever;\n    this.chatHistory = [];\n  }\n\n  async query(question) {\n    // Get relevant documents\n    const docs = await this.retriever.getRelevantDocuments(question);\n    const context = docs.map(doc => doc.pageContent).join('\\n\\n');\n    \n    // Format chat history\n    const chatHistoryStr = this.chatHistory\n      .map(msg => `${msg.type}: ${msg.content}`)\n      .join('\\n');\n\n    // Create chain\n    const chain = RunnableSequence.from([\n      conversationalPrompt,\n      this.llm,\n      new StringOutputParser(),\n    ]);\n\n    // Get response\n    const response = await chain.invoke({\n      question,\n      context,\n      chat_history: chatHistoryStr,\n    });\n\n    // Update chat history\n    this.chatHistory.push(\n      { type: 'Human', content: question },\n      { type: 'Assistant', content: response }\n    );\n\n    // Keep only last 10 exchanges\n    if (this.chatHistory.length > 20) {\n      this.chatHistory = this.chatHistory.slice(-20);\n    }\n\n    return {\n      answer: response,\n      sourceDocuments: docs,\n      chatHistory: this.chatHistory,\n    };\n  }\n\n  clearHistory() {\n    this.chatHistory = [];\n  }\n}\n\n// Usage\nconst conversationalRag = new ConversationalRAG(llm, retriever);\n\n// First question\nconst response1 = await conversationalRag.query('What is machine learning?');\nconsole.log('Response 1:', response1.answer);\n\n// Follow-up question (uses context from previous conversation)\nconst response2 = await conversationalRag.query('Can you give me more details about the types you mentioned?');\nconsole.log('Response 2:', response2.answer);",
      "demoFunction": "conversationalRagDemo",
      "demoFile": "src/demos/rag/conversational-rag.js",
      "capabilities": {
        "whatItCanDo": [
          "Maintain conversation context across multiple RAG interactions",
          "Build upon previous questions and answers in document retrieval",
          "Provide contextual follow-up responses using chat history",
          "Handle conversational references and pronouns effectively",
          "Create continuous, natural dialogue experiences with knowledge retrieval"
        ],
        "bestUseCases": [
          "Interactive document exploration and research",
          "Conversational knowledge base assistants",
          "Educational tutoring with document support",
          "Customer support with knowledge continuity",
          "Long-form research and analysis sessions"
        ],
        "limitations": [
          "Chat history management complexity increases over time",
          "Context window limitations with long conversations",
          "Potential for context drift in extended sessions",
          "Higher computational overhead due to history processing"
        ],
        "keyTakeaways": [
          "Essential for natural conversational AI experiences",
          "Transforms RAG from single-shot to continuous interaction",
          "Memory management is crucial for performance",
          "Enables sophisticated follow-up questioning",
          "Foundation for advanced conversational AI assistants"
        ]
      }
    },
    {
      "id": "multi-modal-rag",
      "title": "Multi-Modal RAG",
      "description": "RAG system that processes text, images, and structured data",
      "category": "rag",
      "codeSnippet": "import { UnstructuredLoader } from 'langchain/document_loaders/fs/unstructured';\nimport { JSONLoader } from 'langchain/document_loaders/fs/json';\nimport { WebPDFLoader } from 'langchain/document_loaders/web/pdf';\n\n// Multi-modal document processing\nclass MultiModalRAG {\n  constructor(llm, embeddings) {\n    this.llm = llm;\n    this.embeddings = embeddings;\n    this.vectorStores = {\n      text: null,\n      structured: null,\n      metadata: null,\n    };\n  }\n\n  async processDocuments(documents) {\n    const textDocs = [];\n    const structuredDocs = [];\n    const imageDocs = [];\n\n    for (const doc of documents) {\n      if (doc.metadata.type === 'text') {\n        textDocs.push(doc);\n      } else if (doc.metadata.type === 'json' || doc.metadata.type === 'csv') {\n        structuredDocs.push(doc);\n      } else if (doc.metadata.type === 'image') {\n        imageDocs.push(doc);\n      }\n    }\n\n    // Create separate vector stores for different content types\n    if (textDocs.length > 0) {\n      this.vectorStores.text = await MemoryVectorStore.fromDocuments(\n        textDocs,\n        this.embeddings\n      );\n    }\n\n    if (structuredDocs.length > 0) {\n      // Process structured data differently\n      const processedStructured = structuredDocs.map(doc => ({\n        ...doc,\n        pageContent: this.formatStructuredData(doc.pageContent),\n      }));\n      \n      this.vectorStores.structured = await MemoryVectorStore.fromDocuments(\n        processedStructured,\n        this.embeddings\n      );\n    }\n\n    console.log('Multi-modal processing complete:', {\n      textDocuments: textDocs.length,\n      structuredDocuments: structuredDocs.length,\n      imageDocuments: imageDocs.length,\n    });\n  }\n\n  formatStructuredData(jsonContent) {\n    try {\n      const data = JSON.parse(jsonContent);\n      if (Array.isArray(data)) {\n        return data.map(item => \n          Object.entries(item)\n            .map(([key, value]) => `${key}: ${value}`)\n            .join(', ')\n        ).join('\\n');\n      }\n      return Object.entries(data)\n        .map(([key, value]) => `${key}: ${JSON.stringify(value)}`)\n        .join('\\n');\n    } catch {\n      return jsonContent;\n    }\n  }\n\n  async hybridSearch(query, options = {}) {\n    const results = [];\n    const { includeText = true, includeStructured = true, k = 4 } = options;\n\n    if (includeText && this.vectorStores.text) {\n      const textResults = await this.vectorStores.text.similaritySearch(query, k);\n      results.push(...textResults.map(doc => ({ ...doc, type: 'text' })));\n    }\n\n    if (includeStructured && this.vectorStores.structured) {\n      const structuredResults = await this.vectorStores.structured.similaritySearch(query, k);\n      results.push(...structuredResults.map(doc => ({ ...doc, type: 'structured' })));\n    }\n\n    // Sort by relevance (simplified)\n    return results.slice(0, k * 2);\n  }\n\n  async query(question, options = {}) {\n    const relevantDocs = await this.hybridSearch(question, options);\n    \n    const context = relevantDocs\n      .map((doc, index) => `[${doc.type.toUpperCase()} - Source ${index + 1}]\\n${doc.pageContent}`)\n      .join('\\n\\n');\n\n    const prompt = `Based on this multi-modal context (text and structured data), answer the question:\n\nContext:\n${context}\n\nQuestion: ${question}\n\nAnswer:`;\n\n    const response = await this.llm.invoke(prompt);\n    \n    return {\n      answer: response.content,\n      sources: relevantDocs.map(doc => ({\n        type: doc.type,\n        source: doc.metadata.source,\n        content: doc.pageContent.substring(0, 200) + '...',\n      })),\n    };\n  }\n}\n\n// Usage\nconst multiModalRag = new MultiModalRAG(llm, embeddings);\nawait multiModalRag.processDocuments(allDocuments);\n\nconst result = await multiModalRag.query('What are the sales trends for Q4?', {\n  includeText: true,\n  includeStructured: true,\n  k: 3,\n});\n\nconsole.log('Multi-modal RAG result:', result);",
      "demoFunction": "multiModalRagDemo",
      "demoFile": "src/demos/rag/multi-modal-rag.js",
      "capabilities": {
        "whatItCanDo": [
          "Process and integrate multiple data types (text, images, structured data)",
          "Create specialized vector stores for different content types",
          "Perform hybrid search across diverse data modalities",
          "Handle complex enterprise data with mixed formats",
          "Enable comprehensive knowledge retrieval from varied sources"
        ],
        "bestUseCases": [
          "Enterprise knowledge systems with diverse data types",
          "Research applications requiring multi-format analysis",
          "Document management systems with mixed content",
          "Business intelligence with structured and unstructured data",
          "Comprehensive data analysis and reporting systems"
        ],
        "limitations": [
          "Increased complexity in data processing and storage",
          "Higher computational and storage requirements",
          "Challenging to balance relevance across different modalities",
          "Requires specialized handling for each data type"
        ],
        "keyTakeaways": [
          "Represents the future of comprehensive knowledge systems",
          "Essential for handling real-world enterprise data complexity",
          "Requires careful architecture for optimal performance",
          "Enables truly comprehensive AI-powered data analysis",
          "Foundation for next-generation knowledge management"
        ]
      }
    }
  ],
  "workshop": [
    {
      "id": "travel-prompt-basic",
      "title": "Travel Prompt Template",
      "description": "Build your first travel planning prompt with multiple variables",
      "category": "workshop",
      "codeSnippet": "const travelPrompt = new PromptTemplate({\n    template: `You are an expert travel planner. Create a personalized {duration} travel itinerary for {destination}.\n\nTraveler Profile:\n- Budget: {budget}\n- Travel Style: {travelStyle}\n- Interests: {interests}\n- Group Size: {groupSize}\n- Special Requirements: {specialRequirements}\n\nPlease provide:\n1. Daily itinerary with activities\n2. Accommodation recommendations\n3. Transportation suggestions\n4. Budget breakdown\n5. Local tips and cultural insights\n\nFormat your response as a detailed, actionable travel plan.`,\n    inputVariables: [\n        'duration', 'destination', 'budget', 'travelStyle', \n        'interests', 'groupSize', 'specialRequirements'\n    ],\n});\n\nconst formattedPrompt = await travelPrompt.format({\n    duration: '5-day',\n    destination: 'Tokyo, Japan',\n    budget: '$2000 per person',\n    travelStyle: 'Cultural Explorer',\n    interests: 'Food, Temples, Technology',\n    groupSize: '2 adults',\n    specialRequirements: 'Vegetarian meals preferred'\n});",
      "demoFunction": "travelPromptBasicDemo",
      "demoFile": "src/demos/travel-planner/travel-prompt-basic.js",
      "capabilities": {
        "whatItCanDo": [
          "Create comprehensive travel planning prompts with multiple variables",
          "Handle complex user preferences and requirements",
          "Generate structured travel itinerary requests",
          "Support various travel styles and group configurations",
          "Provide consistent format for AI travel interactions"
        ],
        "bestUseCases": [
          "AI-powered travel planning applications",
          "Personalized itinerary generation systems",
          "Travel recommendation engines",
          "Custom travel assistant chatbots",
          "Workshop prompt engineering demonstrations"
        ],
        "limitations": [
          "Output quality depends on the underlying language model",
          "Requires careful prompt engineering for optimal results",
          "May need additional validation for travel data accuracy",
          "Complex prompts can be harder to debug and maintain"
        ],
        "keyTakeaways": [
          "Foundation for all travel AI applications",
          "Demonstrates advanced prompt template techniques",
          "Shows how to handle multiple complex variables",
          "Perfect starting point for travel assistant development",
          "Essential skill for AI application builders"
        ]
      }
    },
    {
      "id": "travel-chain-basic",
      "title": "Simple Trip Suggestion Pipeline",
      "description": "Build a basic chain that processes travel requests and generates suggestions",
      "category": "workshop",
      "codeSnippet": "import { PromptTemplate } from '@langchain/core/prompts';\nimport { RunnableSequence } from '@langchain/core/runnables';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\nimport { OpenAI } from '@langchain/openai';\n\n// Step 1: Destination Analysis\nconst destinationAnalysisPrompt = new PromptTemplate({\n    template: `Analyze the travel destination: {destination}\nBudget: {budget}\nTravel Style: {travelStyle}\n\nProvide:\n1. Best time to visit\n2. Budget feasibility analysis\n3. Top 3 must-see attractions\n4. Local transportation options\n\nKeep response concise and factual.`,\n    inputVariables: ['destination', 'budget', 'travelStyle']\n});\n\n// Step 2: Itinerary Generation\nconst itineraryPrompt = new PromptTemplate({\n    template: `Based on this destination analysis:\n{analysis}\n\nCreate a {duration} itinerary for {destination}.\nInterests: {interests}\nGroup: {groupSize}\n\nProvide day-by-day activities with timing and costs.`,\n    inputVariables: ['analysis', 'duration', 'destination', 'interests', 'groupSize']\n});\n\n// Create the chain\nconst travelPlanningChain = RunnableSequence.from([\n    // Step 1: Analyze destination\n    {\n        analysis: destinationAnalysisPrompt.pipe(model).pipe(new StringOutputParser()),\n        destination: (input) => input.destination,\n        duration: (input) => input.duration,\n        interests: (input) => input.interests,\n        groupSize: (input) => input.groupSize\n    },\n    // Step 2: Generate itinerary\n    itineraryPrompt.pipe(model).pipe(new StringOutputParser())\n]);",
      "demoFunction": "travelChainBasicDemo",
      "demoFile": "src/demos/travel-planner/travel-chain-basic.js",
      "capabilities": {
        "whatItCanDo": [
          "Process travel requests through multiple analysis steps",
          "Combine destination analysis with itinerary generation",
          "Handle complex data flow between chain components",
          "Generate comprehensive travel plans automatically",
          "Create reusable travel planning workflows"
        ],
        "bestUseCases": [
          "Automated travel planning systems",
          "Multi-step travel recommendation engines",
          "Travel agency automation tools",
          "Personalized trip planning applications",
          "Educational chain building demonstrations"
        ],
        "limitations": [
          "Requires careful chain design for optimal results",
          "More complex debugging than single prompts",
          "Higher API costs due to multiple LLM calls",
          "Potential for error propagation between steps"
        ],
        "keyTakeaways": [
          "Chains enable sophisticated multi-step workflows",
          "Essential for building complex travel applications",
          "Demonstrates data flow management in LangChain",
          "Foundation for advanced travel planning systems",
          "Shows practical application of chain composition"
        ]
      }
    },
    {
      "id": "travel-chain-structured",
      "title": "Structured Travel Chain",
      "description": "Build travel chains with structured JSON output for production applications",
      "category": "workshop",
      "codeSnippet": "import { PromptTemplate } from '@langchain/core/prompts';\nimport { RunnableSequence } from '@langchain/core/runnables';\nimport { JsonOutputParser } from '@langchain/core/output_parsers';\nimport { z } from 'zod';\n\n// Define structured output schemas with Zod validation\nconst destinationAnalysisSchema = z.object({\n  destination: z.string().describe('The destination name'),\n  bestTimeToVisit: z.object({\n    months: z.array(z.string()).describe('Best months to visit'),\n    season: z.string().describe('Best season'),\n    weather: z.string().describe('Expected weather conditions')\n  }),\n  budgetAnalysis: z.object({\n    feasibility: z.enum(['Low', 'Medium', 'High']).describe('Budget feasibility'),\n    dailyBudget: z.object({\n      budget: z.string().describe('Recommended daily budget range'),\n      accommodation: z.string().describe('Accommodation cost range'),\n      food: z.string().describe('Food cost range')\n    })\n  })\n});\n\nconst itinerarySchema = z.object({\n  destination: z.string().describe('Destination name'),\n  duration: z.string().describe('Trip duration'),\n  totalBudget: z.string().describe('Total estimated budget'),\n  dailyItinerary: z.array(z.object({\n    day: z.number().describe('Day number'),\n    theme: z.string().describe('Day theme or focus'),\n    activities: z.array(z.object({\n      time: z.string().describe('Activity time'),\n      activity: z.string().describe('Activity name'),\n      location: z.string().describe('Activity location'),\n      cost: z.string().describe('Estimated cost')\n    }))\n  })),\n  packingList: z.array(z.string()).describe('Essential items to pack'),\n  culturalTips: z.array(z.string()).describe('Cultural etiquette and tips')\n});\n\n// Create JSON output parsers with schema validation\nconst analysisParser = new JsonOutputParser({ schema: destinationAnalysisSchema });\nconst itineraryParser = new JsonOutputParser({ schema: itinerarySchema });\n\n// Build structured chain with proper schema validation\nconst structuredChain = RunnableSequence.from([\n  // Step 1: Structured destination analysis\n  {\n    analysis: structuredAnalysisPrompt.pipe(model).pipe(analysisParser),\n    destination: (input) => input.destination,\n    duration: (input) => input.duration\n  },\n  // Step 2: Structured itinerary generation\n  (input) => ({\n    ...input,\n    analysis: JSON.stringify(input.analysis, null, 2)\n  }),\n  structuredItineraryPrompt.pipe(model).pipe(itineraryParser)\n]);",
      "demoFunction": "travelChainStructuredDemo",
      "demoFile": "src/demos/travel-planner/travel-chain-structured.js",
      "capabilities": {
        "whatItCanDo": [
          "Generate structured JSON output from AI models",
          "Define and validate complex data schemas with Zod",
          "Create type-safe travel planning responses",
          "Build production-ready chains with consistent output formats",
          "Handle structured data parsing and validation"
        ],
        "bestUseCases": [
          "Production travel applications requiring consistent data",
          "Frontend integration with predictable API responses",
          "Database storage with standardized schemas",
          "Automated travel planning workflows",
          "Type-safe application development"
        ],
        "limitations": [
          "Requires careful schema design and maintenance",
          "JSON parsing can fail with malformed AI responses",
          "More complex prompt engineering needed",
          "Higher token usage due to detailed format instructions",
          "Schema changes require coordinated updates"
        ],
        "keyTakeaways": [
          "Structured output is essential for production AI applications",
          "Schema validation prevents runtime errors",
          "JSON format enables seamless frontend integration",
          "Type safety improves development experience",
          "Critical skill for building reliable AI systems"
        ]
      }
    },
    {
      "id": "travel-knowledge-base",
      "title": "Travel Knowledge Base (RAG)",
      "description": "Build a knowledge-powered travel assistant using Retrieval-Augmented Generation",
      "category": "workshop",
      "codeSnippet": "import { MemoryVectorStore } from 'langchain/vectorstores/memory';\nimport { OpenAIEmbeddings } from '@langchain/openai';\nimport { Document } from '@langchain/core/documents';\n\n// Create travel knowledge documents\nconst travelKnowledge = [\n  new Document({\n    pageContent: `Tokyo, Japan is a vibrant metropolis blending traditional culture with cutting-edge technology. Best time to visit is March-May (spring) and September-November (autumn). Must-see attractions include Senso-ji Temple, Tokyo Skytree, Shibuya Crossing...`,\n    metadata: { destination: 'Tokyo', country: 'Japan', category: 'overview' }\n  }),\n  // ... more destinations\n];\n\n// Initialize embeddings and vector store\nconst embeddings = new OpenAIEmbeddings({\n  apiKey: process.env.OPENAI_API_KEY,\n  modelName: 'text-embedding-ada-002',\n});\n\nconst vectorStore = await MemoryVectorStore.fromDocuments(\n  travelKnowledge,\n  embeddings\n);\n\nconst retriever = vectorStore.asRetriever({ k: 3 });\n\n// Create RAG chain\nconst ragChain = RunnableSequence.from([\n  {\n    context: async (input) => {\n      const docs = await retriever.getRelevantDocuments(input.question);\n      return docs.map(doc => doc.pageContent).join('\\n\\n');\n    },\n    question: (input) => input.question\n  },\n  ragPrompt,\n  model,\n  new StringOutputParser()\n]);",
      "demoFunction": "travelKnowledgeBaseDemo",
      "demoFile": "src/demos/travel-planner/travel-knowledge-base.js",
      "capabilities": {
        "whatItCanDo": [
          "Store and retrieve travel knowledge from multiple destinations",
          "Perform semantic search across travel documents",
          "Generate context-aware travel recommendations",
          "Combine external knowledge with AI language models",
          "Build domain-specific knowledge bases for travel planning"
        ],
        "bestUseCases": [
          "Knowledge-powered travel recommendation systems",
          "Travel chatbots with factual destination information",
          "Personalized itinerary generation with local insights",
          "Travel agency knowledge management systems",
          "Educational RAG implementation demonstrations"
        ],
        "limitations": [
          "Requires vector database setup and maintenance",
          "Knowledge base needs regular updates for accuracy",
          "Embedding costs can scale with document volume",
          "Retrieval quality depends on document chunking strategy",
          "May retrieve irrelevant context for ambiguous queries"
        ],
        "keyTakeaways": [
          "RAG enables AI to access external knowledge sources",
          "Vector embeddings enable semantic document search",
          "Context retrieval significantly improves AI accuracy",
          "Knowledge bases require careful curation and organization",
          "Essential pattern for building knowledge-aware AI applications"
        ]
      }
    },
    {
      "id": "travel-agent-tools",
      "title": "Travel Agent with Tools",
      "description": "Build an intelligent travel agent that uses external tools for decision-making",
      "category": "workshop",
      "codeSnippet": "import { ChatOpenAI } from '@langchain/openai';\nimport { AgentExecutor, createReactAgent } from 'langchain/agents';\nimport { Tool } from '@langchain/core/tools';\n\n// Custom Travel Tools\nclass WeatherTool extends Tool {\n  name = 'weather_checker';\n  description = 'Get current weather conditions and forecast for a travel destination';\n\n  async _call(destination) {\n    // Simulate weather API call\n    return `Weather in ${destination}: 22Â°C, Partly Cloudy, Perfect for sightseeing!`;\n  }\n}\n\nclass CurrencyTool extends Tool {\n  name = 'currency_converter';\n  description = 'Convert currency for travel budgeting';\n\n  async _call(query) {\n    // Simulate currency conversion\n    return `ðŸ’± 1000 USD = 150,250 JPY (Rate: 150.25)`;\n  }\n}\n\n// Initialize tools and agent\nconst tools = [new WeatherTool(), new CurrencyTool()];\nconst model = new ChatOpenAI({ temperature: 0.7 });\n\nconst agent = await createReactAgent({\n  llm: model,\n  tools,\n  prompt: agentPrompt\n});\n\nconst agentExecutor = new AgentExecutor({\n  agent,\n  tools,\n  verbose: true,\n  maxIterations: 5\n});\n\n// Execute travel planning query\nconst result = await agentExecutor.invoke({\n  input: 'Plan a trip to Tokyo, check weather and convert 2000 USD to JPY'\n});",
      "demoFunction": "travelAgentToolsDemo",
      "demoFile": "src/demos/travel-planner/travel-agent-tools.js",
      "capabilities": {
        "whatItCanDo": [
          "Create intelligent agents that use external tools",
          "Implement ReAct (Reasoning + Acting) pattern for decision-making",
          "Orchestrate multiple tools for complex travel planning tasks",
          "Simulate real-world API integrations (weather, currency, booking)",
          "Build multi-step reasoning workflows with tool selection"
        ],
        "bestUseCases": [
          "Intelligent travel planning assistants",
          "Multi-service travel booking platforms",
          "Automated travel research and comparison tools",
          "Personal travel concierge applications",
          "Educational agent development workshops"
        ],
        "limitations": [
          "Requires careful tool design and error handling",
          "Agent reasoning quality depends on LLM capabilities",
          "Tool execution can be slower than direct API calls",
          "Complex agent behaviors can be difficult to debug",
          "Real API integrations require proper authentication and rate limiting"
        ],
        "keyTakeaways": [
          "Agents enable AI to interact with external systems",
          "Tool-based architecture provides extensible functionality",
          "ReAct pattern combines reasoning with action execution",
          "Essential for building practical AI applications",
          "Foundation for autonomous AI system development"
        ]
      }
    },
    {
      "id": "travel-memory-sessions",
      "title": "Memory Management & User Sessions",
      "description": "Build personalized AI with memory that remembers user preferences and conversation history",
      "category": "workshop",
      "codeSnippet": "import { BufferMemory, ConversationSummaryMemory } from 'langchain/memory';\nimport { ConversationChain } from 'langchain/chains';\n\n// Custom Travel Memory Manager\nclass TravelMemoryManager {\n  constructor() {\n    this.userSessions = new Map();\n    this.userProfiles = new Map();\n    this.conversationHistory = new Map();\n  }\n\n  getOrCreateSession(userId) {\n    if (!this.userSessions.has(userId)) {\n      this.userSessions.set(userId, {\n        id: userId,\n        createdAt: new Date(),\n        preferences: {},\n        memory: new BufferMemory({\n          memoryKey: 'chat_history',\n          returnMessages: true\n        })\n      });\n    }\n    return this.userSessions.get(userId);\n  }\n\n  updateUserProfile(userId, profileData) {\n    const profile = { ...this.userProfiles.get(userId), ...profileData };\n    this.userProfiles.set(userId, profile);\n    return profile;\n  }\n}\n\n// Memory-enhanced conversation chain\nconst memoryManager = new TravelMemoryManager();\nconst session = memoryManager.getOrCreateSession('user_001');\n\nconst chain = new ConversationChain({\n  llm: model,\n  memory: session.memory,\n  prompt: personalizedPrompt\n});\n\nconst response = await chain.call({\n  input: 'Plan a trip based on my previous preferences'\n});",
      "demoFunction": "travelMemorySessionsDemo",
      "demoFile": "src/demos/travel-planner/travel-memory-sessions.js",
      "capabilities": {
        "whatItCanDo": [
          "Manage user sessions and maintain conversation context",
          "Learn and store user travel preferences automatically",
          "Provide personalized recommendations based on history",
          "Handle multiple concurrent user sessions",
          "Implement different memory types (Buffer, Summary, Entity)"
        ],
        "bestUseCases": [
          "Personalized travel planning applications",
          "Multi-user travel platforms with user accounts",
          "Conversational travel assistants with context retention",
          "Travel recommendation systems with learning capabilities",
          "Customer service applications with conversation history"
        ],
        "limitations": [
          "Memory storage can grow large with extensive conversations",
          "Requires careful privacy and data retention management",
          "Memory persistence needs database integration for production",
          "Preference extraction accuracy depends on conversation quality",
          "Session management complexity increases with scale"
        ],
        "keyTakeaways": [
          "Memory enables personalized and context-aware AI interactions",
          "User session management is crucial for multi-user applications",
          "Automatic preference learning improves user experience over time",
          "Different memory types serve different use cases and constraints",
          "Essential component for building production-ready conversational AI"
        ]
      }
    },
    {
      "id": "smart-travel-planner-complete",
      "title": "ðŸŒŸ Complete Smart Travel Planner",
      "description": "Capstone demo integrating ALL workshop components: Prompts + Chains + Structured Output + RAG + Agents + Memory",
      "category": "workshop",
      "codeSnippet": "// COMPLETE SMART TRAVEL PLANNER INTEGRATION\n// Combines all workshop components into one comprehensive system\n\nclass SmartTravelPlanner {\n  constructor() {\n    this.userSessions = new Map();\n    this.knowledgeBase = null;\n    this.tools = [];\n    this.model = null;\n  }\n\n  async initialize() {\n    // Initialize AI model, knowledge base, and tools\n    await this.initializeKnowledgeBase();\n    this.initializeTools();\n  }\n\n  async planTravel(userId, request) {\n    // Step 1: Get or create user session (Memory)\n    const session = this.getOrCreateUserSession(userId);\n    \n    // Step 2: Extract preferences (Prompt Templates)\n    const preferences = this.extractPreferences(request);\n    \n    // Step 3: Retrieve knowledge (RAG)\n    const knowledge = await this.retrieveKnowledge(request);\n    \n    // Step 4: Execute tools (Agents)\n    const weatherInfo = await this.tools[0]._call(destination);\n    \n    // Step 5: Generate structured itinerary (Chains + Structured Output)\n    const itinerary = await this.generateStructuredItinerary(\n      destination, duration, budget, interests, groupSize\n    );\n    \n    // Step 6: Create comprehensive response\n    return {\n      userProfile: session.profile,\n      travelPlan: { destination, duration, budget },\n      knowledge,\n      currentConditions: { weather: weatherInfo },\n      structuredItinerary: itinerary,\n      recommendations: this.generatePersonalizedRecommendations()\n    };\n  }\n}\n\n// Demo: Complete workflow with 3 scenarios\nconst planner = new SmartTravelPlanner();\nawait planner.initialize();\n\n// Scenario 1: Adventure trip to Costa Rica\nconst plan1 = await planner.planTravel('user_alice', \n  'I want a 5-day adventure trip to Costa Rica with $2000 budget');\n\n// Scenario 2: Luxury Paris getaway\nconst plan2 = await planner.planTravel('user_bob',\n  'Plan a luxury romantic getaway to Paris for 3 days');\n\n// Scenario 3: Returning user with memory\nconst plan3 = await planner.planTravel('user_alice',\n  'What other adventure destinations would you recommend?');",
      "demoFunction": "smartTravelPlannerCompleteDemo",
      "demoFile": "src/demos/travel-planner/smart-travel-planner-complete.js",
      "capabilities": {
        "whatItCanDo": [
          "Integrate all LangChain components into one comprehensive system",
          "Manage complete user journey from first interaction to booking",
          "Combine prompt templates, chains, structured output, RAG, agents, and memory",
          "Handle multiple user sessions with personalized experiences",
          "Demonstrate production-ready AI application architecture",
          "Execute multi-step workflows with data flow between components",
          "Generate structured JSON outputs for frontend integration",
          "Provide real-time travel information through agent tools"
        ],
        "bestUseCases": [
          "Complete AI travel planning applications",
          "Enterprise-grade conversational AI systems",
          "Multi-user platforms with personalization",
          "Production AI applications requiring all LangChain features",
          "Educational demonstrations of AI application architecture",
          "Proof-of-concept for comprehensive AI travel assistants",
          "Workshop capstone projects integrating multiple AI concepts"
        ],
        "limitations": [
          "Complexity increases maintenance overhead",
          "Requires careful orchestration of multiple components",
          "Memory and performance considerations with all features active",
          "Error handling complexity across integrated systems",
          "Production deployment requires infrastructure for all components",
          "API costs increase with multiple AI service calls",
          "Testing complexity with integrated system dependencies"
        ],
        "keyTakeaways": [
          "Integration of all LangChain components creates powerful AI applications",
          "Component orchestration is crucial for complex AI system success",
          "User session management enables personalized AI experiences",
          "Structured workflows provide predictable and reliable AI outputs",
          "Production-ready AI requires careful architecture and error handling",
          "Complete AI systems demonstrate the full potential of LangChain",
          "Essential capstone for understanding comprehensive AI application development"
        ]
      }
    }
  ]
}
